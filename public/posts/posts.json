[
  {
    "path": "posts/2022-01-31-xgboost-as-a-feature-generator/",
    "title": "XGBoost as an Interaction Explorer",
    "description": "This post explores one approach to transforming tree-based models, in this case XGBoost, into linear models. In this framework, XGBoost can be seen as a method for exploring interactions between features and regions of data.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2022-01-31",
    "categories": [],
    "contents": "\r\nIntroduction\r\nTree-based models, like XGBoost and Random Forests, can be incredibly powerful tools for modeling structured data, however they are are not typically considered interpretable. While you can inspect the trees, look at feature importance, or use approaches like LIME and SHAP, but these approaches are not perfect (Slack et al. 2020) and are often not sufficient for high-stakes decisions (Rudin 2019). In this post, I will one approach for using XGBoost to generate features and explore interactions between variables and regions of data that can then be used in an interpretable linear model.\r\nThis is not an original idea by any means! I first learned of this approach when reading about the Loft Data Science Team’s XGBoost Survival Embeddings (Loft 2021). The Scikit-learn documentation also discusses a similar approach, and shows how it can be incorporated in a pipeline (Scikit-learn 2021).\r\nThis post presumes some knowledge about decision trees, but not necessarily any specific tree-growing method.\r\nDefinitions\r\nI’ll start by definining a few terms that I may be using in a non-standard way, or that may be unfamiliar.\r\nLeaves: Leaves are the final nodes in a decision tree. Every data point gets sorted into the leaf of a decision tree. Leaves can be seen as representing the interaction between every variable used in the preceeding nodes.\r\nInteractions: An interaction refers to relationship between two variables in relation to the outcome. These are often modeled explicitly in linear models (Wikipedia, n.d.), but XGboost’s tree building can be seen as discovering useful interactions between variables and crucially different values of those variables.\r\nRegions of data: Decision trees make decisions by choosing a splitting variable, and then a value at which to split. Sucessive splits can be seen as not just interactions between variables, but as interactions between regions of the data.\r\nEmbeddings: The leaves of a decision tree are sometimes called embeddings.\r\nR Setup\r\nI’ll be using R for this blog post, but the approaches described here are language agnostic. I did not find it any more or less painful to write this in Python.\r\n\r\n\r\n# Data manipulation\r\nlibrary(dplyr)\r\nlibrary(tidyr)\r\n# I/O\r\nlibrary(readr)\r\n# Modeling\r\nlibrary(tidymodels)\r\nlibrary(xgboost)\r\nlibrary(vip)\r\n\r\n# Settings\r\ntidymodels_prefer()\r\n\r\n\r\n\r\nData Setup\r\nI’ll be copying Julia Silge and using the Sliced Semifinals dataset. A nice bonus: we can compare our models against the SLICED contestants and viewers! The goal is to predict binned sale price given home features; the binned sale prices mean that this is a multiclass classification problem.\r\n\r\n\r\n# Read data\r\ndf_train <- read_csv('data/train.csv')\r\ndf_test <- read_csv('data/test.csv')\r\n\r\n# Split into sets\r\ndf_train_split <- initial_split(df_train, strata = priceRange)\r\ndf_train <- training(df_train_split)\r\ndf_train_folds <- vfold_cv(df_train, strata = priceRange)\r\ndf_valid <- testing(df_train_split)\r\n\r\n\r\n\r\nThe evaluation metric is multiclass log-loss, but I’ll also include accuracy and (multiclass) ROC AUC in the metric set. Note that the first metric in a {yardstick} metric set is the one used when hyperparameter tuning.\r\n\r\n\r\nmc_metrics <- metric_set(mn_log_loss, roc_auc, accuracy)\r\n\r\n\r\n\r\nLet’s take a quick look at the data.\r\n\r\n\r\nskimr::skim(df_train) %>% \r\n  select(-numeric.hist, -starts_with('numeric.p'))\r\n\r\n\r\nTable 1: Data summary\r\nName\r\ndf_train\r\nNumber of rows\r\n7498\r\nNumber of columns\r\n16\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\ncharacter\r\n4\r\nlogical\r\n1\r\nnumeric\r\n11\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: character\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmin\r\nmax\r\nempty\r\nn_unique\r\nwhitespace\r\ncity\r\n0\r\n1\r\n6\r\n16\r\n0\r\n6\r\n0\r\ndescription\r\n1\r\n1\r\n1\r\n3988\r\n0\r\n7488\r\n2\r\nhomeType\r\n0\r\n1\r\n5\r\n21\r\n0\r\n10\r\n0\r\npriceRange\r\n0\r\n1\r\n7\r\n13\r\n0\r\n5\r\n0\r\nVariable type: logical\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\ncount\r\nhasSpa\r\n0\r\n1\r\n0.08\r\nFAL: 6894, TRU: 604\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\nuid\r\n0\r\n1\r\n7605.85\r\n4384.99\r\nlatitude\r\n0\r\n1\r\n30.29\r\n0.10\r\nlongitude\r\n0\r\n1\r\n-97.78\r\n0.09\r\ngarageSpaces\r\n0\r\n1\r\n1.22\r\n1.32\r\nyearBuilt\r\n0\r\n1\r\n1988.59\r\n21.50\r\nnumOfPatioAndPorchFeatures\r\n0\r\n1\r\n0.66\r\n0.98\r\nlotSizeSqFt\r\n0\r\n1\r\n17908.62\r\n321366.21\r\navgSchoolRating\r\n0\r\n1\r\n5.76\r\n1.85\r\nMedianStudentsPerTeacher\r\n0\r\n1\r\n14.86\r\n1.75\r\nnumOfBathrooms\r\n0\r\n1\r\n2.69\r\n0.98\r\nnumOfBedrooms\r\n0\r\n1\r\n3.45\r\n0.81\r\n\r\nWhile likely very useful, I’m going to ignore the city, description, and homeType columns. I just don’t want to focus on text processing, and I don’t want to worry about low frequency factor values.\r\nBaseline Linear Model\r\nLet’s set the baseline by fitting a multinomial logistic regression. I’m going to ignore latitude and longitude for this baseline model, as I don’t think it’s reasonable to assume that price should change linearly with location alone, but maybe I don’t know enough about Austin.\r\nSet the recipe.\r\n\r\n\r\n# mlr_recipe <- recipe(priceRange ~ )\r\n\r\n\r\n\r\nXGboost\r\nHyperparameter Tuning\r\nExtracting Leaves from XGBoost\r\nFinal Model\r\nEmbeddings Alone\r\nEmbeddings + Features\r\nTest Performance\r\nSome Neat Things\r\nMissing Data\r\nRandom Forests\r\n\r\n\r\n\r\nLoft. 2021. “Xgbse: XGBoost Survival Embeddings.” https://loft-br.github.io/xgboost-survival-embeddings/index.html.\r\n\r\n\r\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” https://arxiv.org/abs/1811.10154.\r\n\r\n\r\nScikit-learn. 2021. “Feature Transformations with Ensembles of Trees.” https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html.\r\n\r\n\r\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. “Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.” https://arxiv.org/abs/1911.02508.\r\n\r\n\r\nWikipedia. n.d. https://en.wikipedia.org/wiki/Interaction_(statistics).\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2022-01-30T15:57:55-08:00",
    "input_file": "xgboost-as-a-feature-generator.knit.md"
  },
  {
    "path": "posts/2020-03-16-what-is-an-improper-prior/",
    "title": "What is an improper prior?",
    "description": "A short introduction to the use of improper priors in Bayesian statistics.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-03-16",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMathJax.Hub.Config({\r\n  TeX: { \r\n      equationNumbers: { \r\n            autoNumber: \"AMS\"\r\n      } \r\n  }\r\n});\r\nIntroduction\r\nWhile I’ve purchased improperprior.com as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.\r\nThis post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.\r\nPriors As Usual\r\nIn the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you’re into Empirical Bayes), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.\r\nThis process is powered by Bayes’ theorem. For example, suppose we have data \\(Y\\) that follow a Binomial distribution, and we would like to estimate the \\(\\theta\\) parameter. Using Bayes’ theorem we would compute:\r\n\\[\\begin{equation}\r\n\\tag{1}\r\nP(\\theta|Y) = \\frac{L(Y|\\theta)P(\\theta)}{P(Y)}.\r\n\\end{equation}\\]\r\nWhere:\r\n\\(P(\\theta|Y)\\) is the posterior probability, this is what we’d like to estimate.\r\n\\(L(Y|\\theta)\\) is the likelihood of observing the data, given \\(\\theta\\).\r\n\\(P(\\theta)\\) is a distribution representing a prior guess for \\(\\theta\\) before observing data.\r\n\\(P(Y)\\) is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a proper distribution), which is surprisingly unnecessary for posterior estimation.\r\nThe canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it’s potential shapes.\r\n\r\n\r\n\r\nFor this example we’ll use a flat prior, which gives equal weight to all possible values of \\(\\theta\\). The \\(\\text{Beta}(1, 1)\\) distribution does this by just giving a Uniform distribution over \\([0, 1]\\).\r\nThe likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just \\(\\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i}\\). We can complete the numerator of (1) by combining this likelihood with the \\(\\text{Beta}(1, 1)\\) prior, which gives:\r\n\\[\\begin{equation}\r\n\\tag{2}\r\nP(\\theta|Y) \\propto \\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i} \\times \\theta^{1-1}(1-\\theta)^{1-1} \\\\\r\n            \\propto \\theta^{\\sum_i y_i + 1 - 1}(1-\\theta)^{n-\\sum_i y_i  + 1 - 1}.\r\n\\end{equation}\\]\r\nFrom this we can see that the posterior distribution follows a \\(\\text{Beta}(\\sum_i y_i + 1, n - \\sum_i y_i + 1)\\) distribution. Intuitively because we used a prior that carried little information with it (effectively saying that \\(\\theta\\) could be any value in (0, 1)) our posterior estimate for \\(\\theta\\) is entirely determined by the data.\r\nHere’s a simple Shiny app to let you play with this model and build some intuition as to how the prior parameters and data interact.\r\n\r\n\r\n\r\n\r\nLimitations\r\nSo far this seems like a great framework, but the requirement that the prior be a proper distribution can be quite restrictive. Consider the normal distribution (for simplicity, assume known \\(\\sigma^2\\)): \\(\\mu\\) can take any real value, so how can we use a flat prior with equal probability for each possible value of \\(\\mu\\)?\r\n\r\nNote that a proper distribution is one with a density function that integrates to 1.\r\nWhile powerful in specific cases, Bayesian modeling is rather limited if we can only use proper distributions as priors.\r\nImproper Priors\r\nNaively, what would happen if we just set the probability of each \\(\\mu\\) to 1? Well it turns out that we can do exactly this - we can use any prior, even an improper prior as long as the posterior comes out to be a proper distribution.\r\nChoosing an improper prior that generates a valid posterior can be a tricky affair, but using Jeffreys’ prior is a good place to start. Continuing the normal example, we will just use a prior probability of 1 for every value of \\(\\mu\\). This is actially proportional to the Jeffreys’ prior for this setup. As with the previous example, we will set aside all constant terms:\r\n\\[\r\nP(\\theta|Y) \\propto \\exp{\\left[-\\frac{1}{2} \\left(\\frac{y-\\mu}{\\sigma^2}\\right)^2\\right]} \\times 1.\r\n\\]\r\nThe posterior is just a proper normal distribution!\r\nIt is of course possible to go much deeper into improper priors, particularly choosing a good prior, but as far as the concept goes, this is mostly all there is to it!\r\nFurther Resources\r\nCraig Gidney has a nice blog post walking through a slightly more technical example of improper priors. Likewise, Andy Jones has a great podcast with a few additional examples. For a more general treatment A First Course in Bayesian Statistical Methods by Hoff and Bayesian Data Analysis by Gelman et al are the standard introductory Bayesian statistics textbooks.\r\nAs ever, Wikipedia has very detailed articles on priors, more suitable for reference than learning:\r\nImproper Priors\r\nJeffreys’ prior\r\nConjugate Priors\r\nFurther Resources\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-20T08:21:32-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-14-continuous-and-binary-dependent-variables/",
    "title": "Continuous and Binary Dependent Variables",
    "description": "An exploration of the use of continuous and binary dependent variables in causal inference.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-02-14",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIn his 2011 paper “Land tenure and investment incentives: Evidence from West Africa,” James Fenske notes that “studies that use binary investment measures…are also less likely to find a statistically significant effect.” He seems to attribute this to (1) small sample sizes, and (2) the lack of nuance associated with binary variables. Fenske argues that continuous measures of investment intensity are better for causal analysis. As Fenske points out continuous variables are usually noisy. For example, imagine asking a farmer how many KGs of fertilizer they applied to each plot of land in the past year versus asking a farmer if they applied any fertilizer to each plot of land in the past year.\r\nWhile small sample sizes can certainly cause estimation issues (frequentist statistics rely heavily on asymptotic results afterall), I’d like to explore why binary variables may be showing up more often as statistically insignificant in this context.\r\nI will be assuming the use of OLS rather than logistic regression, which is somewhat common in the economics literature for binary variables. I will write in terms of fertilizer amount (continuous) /fertilizer use (binary) and a treatment meant to increase fertilizer use, but these results generalize.\r\nR Setup\r\n\r\n\r\n# Data manipulation\r\nlibrary(dplyr)\r\nlibrary(purrr)\r\nlibrary(tidyr)\r\n# Modeling\r\nlibrary(broom)\r\n# Plotting and tables\r\nlibrary(ggplot2)\r\nlibrary(ggcute)\r\nlibrary(knitr)\r\n\r\n# R settings\r\ntheme_set(theme_minimal() + theme(legend.position = \"none\"))\r\ntheme_update(panel.background = element_rect(fill = \"transparent\", \r\n                                             colour = NA),\r\n             plot.background = element_rect(fill = \"transparent\", \r\n                                            colour = NA))\r\n\r\nopts_chunk$set(echo = T, warning = F, message = F, tidy = F,\r\n               fig.width = 8.5, fig.height = 6,\r\n               dev.args = list(bg = \"transparent\"))\r\n\r\n\r\n\r\nInformation Loss\r\nBinarizing a continuous variable, even if noisy, will result in some amount of information loss. As Fenske points out, we go from estimating the amount of fertilizer used to an indicator for any fertilizer use. If few farmers use fertilizer to begin with, then information loss will be low. That is, we can still tell the difference between farmers who fertilize, and those who don’t. However, if most farmers use some amount of fertilizer, then the binarization will make it very difficult to test for any treatment effect.\r\n\r\n\r\n# Simulate information loss\r\nbind_rows(\r\n  tibble(\r\n    y = rpois(1000, 0.5),\r\n    y_bin = ifelse(y > 0, 1, 0),\r\n    lab = \"Low Information Loss\"\r\n  ),\r\n  tibble(\r\n    y = rpois(1000, 2),\r\n    y_bin = ifelse(y > 0, 1, 0),\r\n    lab = \"High Information Loss\"\r\n  )\r\n) %>% \r\n  gather(variable, value, -lab) %>% \r\n  ggplot(aes(value, fill = variable)) +\r\n  geom_bar(position = \"dodge\") +\r\n  facet_wrap(vars(lab)) +\r\n  scale_fill_fairyfloss()\r\n\r\n\r\n\r\n\r\nSimulations\r\nI will be using simulations to explore this issue. Data will be drawn from a Poisson distribution, which has mean equal to its only parameter, lambda. This is quite convenient for simulating treatment effects, as we can easily increase the treatment effect by shifting lambda.\r\n\r\n\r\nsim_settings <- expand_grid(\r\n  # Number of repeats for each group of simulation settings\r\n  m = 1:5,\r\n  # Sample size\r\n  n = seq(100, 3000, 100),\r\n  # Lambda (mean of Poisson)\r\n  lambda = c(0.5, 1, 1.5),\r\n  # Treatment effect in absolute terms\r\n  # (i.e. added to lambda)\r\n  treat_effect = seq(0.01, 0.2, 0.05)\r\n)\r\nsims <- pmap_df(sim_settings, function(m, n, lambda, treat_effect) {\r\n  # Generate data\r\n  treat <- rep(0:1, each = n/2)\r\n  y <- c(rpois(n/2, lambda), rpois(n/2, lambda + treat_effect))\r\n  y_bin <- ifelse(y > 0, 1, 0)\r\n  \r\n  # Run models\r\n  bind_rows(\r\n    lm(y ~ treat) %>% tidy(),\r\n    lm(y_bin ~ treat) %>% tidy()\r\n  ) %>% \r\n    filter(term == \"treat\") %>% \r\n    mutate(m = m, n = n, lambda = lambda, treat_effect = treat_effect,\r\n           outcome = c(\"continuous\", \"binary\"))\r\n})\r\n\r\n\r\n\r\nBinarization will cause issues estimating a precise treatment effect, with higher information loss (i.e. higher lambda) leading to more bias.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(bias = mean(estimate - treat_effect)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, bias, colour = outcome)) +\r\n  geom_hline(yintercept = 0.0) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_fairyfloss() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nWhile effect size estimates are biased, they still seem to work well enough for significance testing in situations where information loss isn’t extreme.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(p.value = mean(p.value)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, p.value, colour = outcome)) +\r\n  geom_hline(yintercept = 0.1) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_fairyfloss() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nConclusion\r\nI suspect Fenske is correct to question the use of binary investment indicators over continuous intensivity indicators. However, it is clear to me that binary indicators are fine for many situations, e.g. measuring the up-take of new agricultural technologies. For situations where the outcome is already common at a low level (and the sample size is high enough), a coarse grained intensivity measure may provide enough information to capture a reasonable estimate of the treatment effect.\r\nSession Info\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\nR version 4.1.0 (2021-05-18)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19042)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United States.1252 \r\n[2] LC_CTYPE=English_United States.1252   \r\n[3] LC_MONETARY=English_United States.1252\r\n[4] LC_NUMERIC=C                          \r\n[5] LC_TIME=English_United States.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n[1] knitr_1.33        ggcute_0.0.0.9000 ggplot2_3.3.5    \r\n[4] broom_0.7.8       tidyr_1.1.3       purrr_0.3.4      \r\n[7] dplyr_1.0.7      \r\n\r\nloaded via a namespace (and not attached):\r\n [1] highr_0.9         pillar_1.6.1      bslib_0.2.5.1    \r\n [4] compiler_4.1.0    jquerylib_0.1.4   tools_4.1.0      \r\n [7] digest_0.6.27     downlit_0.2.1     gtable_0.3.0     \r\n[10] jsonlite_1.7.2    evaluate_0.14     lifecycle_1.0.0  \r\n[13] tibble_3.1.2      pkgconfig_2.0.3   rlang_0.4.11     \r\n[16] DBI_1.1.1         distill_1.2       yaml_2.2.1       \r\n[19] xfun_0.24         withr_2.4.2       stringr_1.4.0    \r\n[22] generics_0.1.0    vctrs_0.3.8       sass_0.4.0       \r\n[25] grid_4.1.0        tidyselect_1.1.1  glue_1.4.2       \r\n[28] R6_2.5.0          fansi_0.5.0       rmarkdown_2.9    \r\n[31] farver_2.1.0      magrittr_2.0.1    scales_1.1.1     \r\n[34] backports_1.2.1   ellipsis_0.3.2    htmltools_0.5.1.1\r\n[37] assertthat_0.2.1  colorspace_2.0-2  labeling_0.4.2   \r\n[40] utf8_1.2.1        stringi_1.6.2     munsell_0.5.0    \r\n[43] crayon_1.4.1     \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-02-14-continuous-and-binary-dependent-variables/continuous-and-binary-dependent-variables_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-20T08:39:22-07:00",
    "input_file": {},
    "preview_width": 1632,
    "preview_height": 1152
  }
]
