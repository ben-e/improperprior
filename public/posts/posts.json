[
  {
    "path": "posts/2020-03-16-what-is-an-improper-prior/",
    "title": "What is an improper prior?",
    "description": "A short introduction to the use of improper priors in Bayesian statistics.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-03-16",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMathJax.Hub.Config({\r\n  TeX: { \r\n      equationNumbers: { \r\n            autoNumber: \"AMS\"\r\n      } \r\n  }\r\n});\r\nIntroduction\r\nWhile I’ve purchased improperprior.com as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.\r\nThis post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.\r\nPriors As Usual\r\nIn the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you’re into Empirical Bayes), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.\r\nThis process is powered by Bayes’ theorem. For example, suppose we have data \\(Y\\) that follow a Binomial distribution, and we would like to estimate the \\(\\theta\\) parameter. Using Bayes’ theorem we would compute:\r\n\\[\\begin{equation}\r\n\\tag{1}\r\nP(\\theta|Y) = \\frac{L(Y|\\theta)P(\\theta)}{P(Y)}.\r\n\\end{equation}\\]\r\nWhere:\r\n\\(P(\\theta|Y)\\) is the posterior probability, this is what we’d like to estimate.\r\n\\(L(Y|\\theta)\\) is the likelihood of observing the data, given \\(\\theta\\).\r\n\\(P(\\theta)\\) is a distribution representing a prior guess for \\(\\theta\\) before observing data.\r\n\\(P(Y)\\) is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a proper distribution), which is surprisingly unnecessary for posterior estimation.\r\nThe canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it’s potential shapes.\r\n\r\n\r\n\r\nFor this example we’ll use a flat prior, which gives equal weight to all possible values of \\(\\theta\\). The \\(\\text{Beta}(1, 1)\\) distribution does this by just giving a Uniform distribution over \\([0, 1]\\).\r\nThe likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just \\(\\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i}\\). We can complete the numerator of (1) by combining this likelihood with the \\(\\text{Beta}(1, 1)\\) prior, which gives:\r\n\\[\\begin{equation}\r\n\\tag{2}\r\nP(\\theta|Y) \\propto \\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i} \\times \\theta^{1-1}(1-\\theta)^{1-1} \\\\\r\n            \\propto \\theta^{\\sum_i y_i + 1 - 1}(1-\\theta)^{n-\\sum_i y_i  + 1 - 1}.\r\n\\end{equation}\\]\r\nFrom this we can see that the posterior distribution follows a \\(\\text{Beta}(\\sum_i y_i + 1, n - \\sum_i y_i + 1)\\) distribution. Intuitively because we used a prior that carried little information with it (effectively saying that \\(\\theta\\) could be any value in (0, 1)) our posterior estimate for \\(\\theta\\) is entirely determined by the data.\r\nHere’s a simple Shiny app to let you play with this model and build some intuition as to how the prior parameters and data interact.\r\n\r\n\r\n\r\n\r\nLimitations\r\nSo far this seems like a great framework, but the requirement that the prior be a proper distribution can be quite restrictive. Consider the normal distribution (for simplicity, assume known \\(\\sigma^2\\)): \\(\\mu\\) can take any real value, so how can we use a flat prior with equal probability for each possible value of \\(\\mu\\)?\r\n\r\nNote that a proper distribution is one with a density function that integrates to 1.\r\nWhile powerful in specific cases, Bayesian modeling is rather limited if we can only use proper distributions as priors.\r\nImproper Priors\r\nNaively, what would happen if we just set the probability of each \\(\\mu\\) to 1? Well it turns out that we can do exactly this - we can use any prior, even an improper prior as long as the posterior comes out to be a proper distribution.\r\nChoosing an improper prior that generates a valid posterior can be a tricky affair, but using Jeffreys’ prior is a good place to start. Continuing the normal example, we will just use a prior probability of 1 for every value of \\(\\mu\\). This is actially proportional to the Jeffreys’ prior for this setup. As with the previous example, we will set aside all constant terms:\r\n\\[\r\nP(\\theta|Y) \\propto \\exp{\\left[-\\frac{1}{2} \\left(\\frac{y-\\mu}{\\sigma^2}\\right)^2\\right]} \\times 1.\r\n\\]\r\nThe posterior is just a proper normal distribution!\r\nIt is of course possible to go much deeper into improper priors, particularly choosing a good prior, but as far as the concept goes, this is mostly all there is to it!\r\nFurther Resources\r\nCraig Gidney has a nice blog post walking through a slightly more technical example of improper priors. Likewise, Andy Jones has a great podcast with a few additional examples. For a more general treatment A First Course in Bayesian Statistical Methods by Hoff and Bayesian Data Analysis by Gelman et al are the standard introductory Bayesian statistics textbooks.\r\nAs ever, Wikipedia has very detailed articles on priors, more suitable for reference than learning:\r\nImproper Priors\r\nJeffreys’ prior\r\nConjugate Priors\r\nFurther Resources\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-20T08:21:32-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-14-continuous-and-binary-dependent-variables/",
    "title": "Continuous and Binary Dependent Variables",
    "description": "An exploration of the use of continuous and binary dependent variables in causal inference.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-02-14",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIn his 2011 paper “Land tenure and investment incentives: Evidence from West Africa,” James Fenske notes that “studies that use binary investment measures…are also less likely to find a statistically significant effect.” He seems to attribute this to (1) small sample sizes, and (2) the lack of nuance associated with binary variables. Fenske argues that continuous measures of investment intensity are better for causal analysis. As Fenske points out continuous variables are usually noisy. For example, imagine asking a farmer how many KGs of fertilizer they applied to each plot of land in the past year versus asking a farmer if they applied any fertilizer to each plot of land in the past year.\r\nWhile small sample sizes can certainly cause estimation issues (frequentist statistics rely heavily on asymptotic results afterall), I’d like to explore why binary variables may be showing up more often as statistically insignificant in this context.\r\nI will be assuming the use of OLS rather than logistic regression, which is somewhat common in the economics literature for binary variables. I will write in terms of fertilizer amount (continuous) /fertilizer use (binary) and a treatment meant to increase fertilizer use, but these results generalize.\r\nR Setup\r\n\r\n\r\n# Data manipulation\r\nlibrary(dplyr)\r\nlibrary(purrr)\r\nlibrary(tidyr)\r\n# Modeling\r\nlibrary(broom)\r\n# Plotting and tables\r\nlibrary(ggplot2)\r\nlibrary(patchwork)\r\nlibrary(ggthemes)\r\nlibrary(knitr)\r\n# R settings\r\ntheme_set(theme_tufte() + theme(legend.position = \"none\"))\r\nopts_chunk$set(echo = T, warning = F, message = F, tidy = F,\r\n               fig.width = 8.5, fig.height = 6)\r\n\r\n\r\n\r\nInformation Loss\r\nBinarizing a continuous variable, even if noisy, will result in some amount of information loss. As Fenske points out, we go from estimating the amount of fertilizer used to an indicator for any fertilizer use. If few farmers use fertilizer to begin with, then information loss will be low. That is, we can still tell the difference between farmers who fertilize, and those who don’t. However, if most farmers use some amount of fertilizer, then the binarization will make it very difficult to test for any treatment effect.\r\n\r\n\r\n# Simulate information loss\r\ntibble(\r\n  y = rpois(1000, 0.5),\r\n  y_bin = ifelse(y > 0, 1, 0)\r\n) %>% \r\n  gather(variable, value) %>% \r\n  ggplot(aes(value, fill = variable)) +\r\n  geom_bar(position = \"dodge\") +\r\n  labs(title = \"Low Information Loss\") +\r\n  scale_fill_few() -> p1\r\ntibble(\r\n  y = rpois(1000, 2),\r\n  y_bin = ifelse(y > 0, 1, 0)\r\n) %>% \r\n  gather(variable, value) %>% \r\n  ggplot(aes(value, fill = variable)) +\r\n  geom_bar(position = \"dodge\") +\r\n  labs(title = \"High Information Loss\") + \r\n  scale_fill_few() -> p2\r\np1 + p2\r\n\r\n\r\n\r\n\r\nSimulations\r\nI will be using simulations to explore this issue. Data will be drawn from a Poisson distribution, which has mean equal to its only parameter, lambda. This is quite convenient for simulating treatment effects, as we can easily increase the treatment effect by shifting lambda.\r\n\r\n\r\nsim_settings <- expand_grid(\r\n  # Number of repeats for each group of simulation settings\r\n  m = 1:5,\r\n  # Sample size\r\n  n = seq(100, 3000, 100),\r\n  # Lambda (mean of Poisson)\r\n  lambda = c(0.5, 1, 1.5),\r\n  # Treatment effect in absolute terms\r\n  # (i.e. added to lambda)\r\n  treat_effect = seq(0.01, 0.2, 0.05)\r\n)\r\nsims <- pmap_df(sim_settings, function(m, n, lambda, treat_effect) {\r\n  # Generate data\r\n  treat <- rep(0:1, each = n/2)\r\n  y <- c(rpois(n/2, lambda), rpois(n/2, lambda + treat_effect))\r\n  y_bin <- ifelse(y > 0, 1, 0)\r\n  \r\n  # Run models\r\n  bind_rows(\r\n    lm(y ~ treat) %>% tidy(),\r\n    lm(y_bin ~ treat) %>% tidy()\r\n  ) %>% \r\n    filter(term == \"treat\") %>% \r\n    mutate(m = m, n = n, lambda = lambda, treat_effect = treat_effect,\r\n           outcome = c(\"continuous\", \"binary\"))\r\n})\r\n\r\n\r\n\r\nBinarization will cause issues estimating a precise treatment effect, with higher information loss (i.e. higher lambda) leading to more bias.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(bias = mean(estimate - treat_effect)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, bias, colour = outcome)) +\r\n  geom_hline(yintercept = 0.0) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_few() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nWhile effect size estimates are biased, they still seem to work well enough for significance testing in situations where information loss isn’t extreme.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(p.value = mean(p.value)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, p.value, colour = outcome)) +\r\n  geom_hline(yintercept = 0.1) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_few() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nConclusion\r\nI suspect Fenske is correct to question the use of binary investment indicators over continuous intensivity indicators. However, it is clear to me that binary indicators are fine for many situations, e.g. measuring the up-take of new agricultural technologies. For situations where the outcome is already common at a low level (and the sample size is high enough), a coarse grained intensivity measure may provide enough information to capture a reasonable estimate of the treatment effect.\r\nSession Info\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\nR version 4.1.0 (2021-05-18)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19042)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United States.1252 \r\n[2] LC_CTYPE=English_United States.1252   \r\n[3] LC_MONETARY=English_United States.1252\r\n[4] LC_NUMERIC=C                          \r\n[5] LC_TIME=English_United States.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n[1] knitr_1.33      ggthemes_4.2.4  patchwork_1.1.1 ggplot2_3.3.5  \r\n[5] broom_0.7.8     tidyr_1.1.3     purrr_0.3.4     dplyr_1.0.7    \r\n\r\nloaded via a namespace (and not attached):\r\n [1] highr_0.9         pillar_1.6.1      bslib_0.2.5.1    \r\n [4] compiler_4.1.0    jquerylib_0.1.4   tools_4.1.0      \r\n [7] digest_0.6.27     downlit_0.2.1     gtable_0.3.0     \r\n[10] jsonlite_1.7.2    evaluate_0.14     lifecycle_1.0.0  \r\n[13] tibble_3.1.2      pkgconfig_2.0.3   rlang_0.4.11     \r\n[16] DBI_1.1.1         distill_1.2       yaml_2.2.1       \r\n[19] xfun_0.24         withr_2.4.2       stringr_1.4.0    \r\n[22] generics_0.1.0    vctrs_0.3.8       sass_0.4.0       \r\n[25] grid_4.1.0        tidyselect_1.1.1  glue_1.4.2       \r\n[28] R6_2.5.0          fansi_0.5.0       rmarkdown_2.9    \r\n[31] farver_2.1.0      magrittr_2.0.1    codetools_0.2-18 \r\n[34] scales_1.1.1      backports_1.2.1   ellipsis_0.3.2   \r\n[37] htmltools_0.5.1.1 assertthat_0.2.1  colorspace_2.0-2 \r\n[40] labeling_0.4.2    utf8_1.2.1        stringi_1.6.2    \r\n[43] munsell_0.5.0     crayon_1.4.1     \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-02-14-continuous-and-binary-dependent-variables/continuous-and-binary-dependent-variables_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-20T08:26:05-07:00",
    "input_file": {},
    "preview_width": 1632,
    "preview_height": 1152
  }
]
