[
  {
    "path": "posts/2022-01-31-xgboost-as-a-feature-generator/",
    "title": "XGBoost as an Interaction Explorer",
    "description": "This post explores one approach to transforming tree-based models, in this case XGBoost, into linear models. In this framework, XGBoost can be seen as a method for exploring interactions between features and regions of data.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2022-01-31",
    "categories": [],
    "contents": "\r\nIntroduction\r\nTree-based models, like XGBoost and Random Forests, can be incredibly powerful tools for modeling structured data, however they are are not typically considered interpretable. While you can inspect the trees, look at feature importance measures and partial-dependence plots, or use explanation methods like LIME and SHAP, these approaches are not perfect (Slack et al. 2020) and are often not sufficient for high-stakes decisions (Rudin 2019) or regulatorily complex environments. In this post, I will show one approach for using XGBoost to generate interpretable embeddings that can be used to improve linear model performance.\r\nThis post presumes some knowledge about decision trees, but not necessarily any specific tree-growing method.\r\nThe Main Idea\r\nA single decision tree is just a set of decision points ending at terminal nodes called a leaf. Within a tree, each data point will be sorted into a specific leaf; each leaf can be thought of as a representation of a partition of the data. This representation naturally includes interactions between features, making it especially powerful. While a single decision tree can be inspected and interpreted directly (just look at each decision), models like XGBoost and Random Forests rely on fitting many trees.\r\nThe general idea presented here is that we can fit XGBoost with relatively shallow trees. Rather than getting predictions from each of the trees, we get the leaf that each data point is represented by. At this point we will have quite a few leaves, exactly how many will depend on the number of trees, depth, and pruning parameters, but it will likely be too many to be considered interpretable. It can be pretty easy to end up with more leaves than observations! To reduce the number of trees we can use a LASSO regression to select the most informative leaves.\r\nWe fit with shallow trees to aid with interpretability, but exactly what constitutes shallow will be subjective (what do you feel like you can interpret correctly?) and may be domain-specific. To prevent overfitting, it’s best to train the XGBoost and LASSO models on separate datasets.\r\nThis is not an original idea by any means! I first learned of this approach when reading about the Loft Data Science Team’s XGBoost Survival Embeddings (Loft 2021). The Scikit-learn documentation also discusses a similar approach, and shows how it can be incorporated in a pipeline (Head 2021).\r\nAn Example\r\nFor the remainder of this post I’ll demonstrate the application of this approach. However, I want to highlight that this approach may not always be the best way to discover interesting embeddings (e.g. as compared to domain knowledge), or may result in no linear model improvement at all.\r\nI’ll be using R for this blog post, but the approach described here is language agnostic. I’ve also done some model tuning that is not included in this post for the sake of focus and brevity.\r\n\r\n\r\n# Data manipulation\r\nlibrary(dplyr)\r\nlibrary(forcats)\r\nlibrary(glue)\r\nlibrary(stringr)\r\nlibrary(tidyr)\r\n# I/O\r\nlibrary(readr)\r\n# Modeling\r\nlibrary(tidymodels)\r\nlibrary(vip)\r\n# Parallel things\r\nlibrary(doParallel)\r\n# Visualization\r\nlibrary(ggplot2)\r\nlibrary(ggthemes)\r\nlibrary(knitr)\r\n\r\n# Settings/misc things\r\ntidymodels_prefer()\r\nregisterDoParallel(parallel::detectCores())\r\nmc_metrics <- metric_set(roc_auc, accuracy)\r\n\r\n\r\n\r\n\r\n\r\n\r\nAustin Housing Data\r\nI’ll be using the Austin housing data used in the Sliced Semifinals dataset. This dataset contains characteristics and binned sale price of 10,000 homes in Austin, TX. The outcome of interest is the sale price, but to make life simpler, I’ll focus on predicting whether or not the price of a given home is above $450,000. While likely very useful, I’m going to ignore the city and description columns, and collapse the homeType column.\r\nLoad the data.\r\n\r\n\r\n# Data is read in elsewhere, because file paths :)\r\n# Encode the outcome and drop uneeded vars\r\ndf <- df %>% \r\n  mutate(y = factor(priceRange %in% c(\"450000-650000\", \"650000+\"),\r\n                    c(T, F), c('above_450000', 'below_450000')),\r\n         homeType = fct_other(homeType, keep = c('Single Family'))) %>%\r\n  select(-c(city, description, priceRange))\r\n\r\n# Split into sets - for XGBoost, regression, and testing\r\ndf_split <- initial_split(df, prop = 0.5, strata = y)\r\ndf_train_split <- initial_split(training(df_split), strata = y)\r\ndf_train_xgb <- training(df_train_split)\r\ndf_train_lr <- testing(df_train_split)\r\ndf_test <- testing(df_split)\r\n\r\n\r\n\r\nBaseline Linear Model\r\nLet’s specify a baseline linear model. Note that because this model is linear and I have no reason to believe that latitude and longitude are linearly related the price range, I am going to omit those variables.\r\n\r\n\r\nlr_recipe <- recipe(y ~ hasSpa + garageSpaces + yearBuilt +\r\n                      numOfPatioAndPorchFeatures + lotSizeSqFt +\r\n                      avgSchoolRating + MedianStudentsPerTeacher + \r\n                      numOfBathrooms + numOfBedrooms + homeType,\r\n                    data = df_train_lr)\r\n\r\nlr_spec <- logistic_reg(mode = 'classification', engine = 'glm')\r\n\r\nlr_wf <- workflow() %>% \r\n  add_recipe(lr_recipe) %>% \r\n  add_model(lr_spec)\r\n\r\n\r\n\r\nNow we can fit and get performance estimates.\r\n\r\n\r\ndf_train_lr_folds <- vfold_cv(df_train_lr, v = 10, strata = y)\r\n\r\nfit_resamples(lr_wf, resamples = df_train_lr_folds) %>%\r\n  collect_metrics() %>% \r\n  kable()\r\n\r\n\r\n.metric\r\n.estimator\r\nmean\r\nn\r\nstd_err\r\n.config\r\naccuracy\r\nbinary\r\n0.7761685\r\n10\r\n0.0048137\r\nPreprocessor1_Model1\r\nroc_auc\r\nbinary\r\n0.8380181\r\n10\r\n0.0059774\r\nPreprocessor1_Model1\r\n\r\nThis is not terrible, but there’s plenty of room for improvement.\r\nFitting XGBoost and Choosing Leaves\r\nWhere is there room for improvement? Well, we omitted a pretty big feature: the location of the home. We did this because there’s no reason to believe, a priori, that home value should increase with latitude or longitude. However, there is reason to believe that latitude and longitude do have some relationship to home price; in other words, neighborhoods exist. Luckily for us, XGBoost is pretty good at learning non-linear relationships. Let’s fit XGBoost using just longitude and latitude, and see if we can effectively use those leaf-embeddings in a downstream model.\r\nSpecify the model. Note that tree depth was set for intepretability, and number of trees is set with my laptop’s performance in mind; the other parameters are the result of some light tuning.\r\n\r\n\r\nlat_lon_recipe <- recipe(y ~ latitude + longitude,\r\n                         data = df_train_xgb)\r\n\r\nxgb_spec <- boost_tree(mode = 'classification', engine = 'xgboost',\r\n                       trees = 100, tree_depth = 2, mtry = 2,\r\n                       min_n = 5, sample_size = 0.412, learn_rate = 0.0997)\r\nxgb_wf <- workflow() %>% \r\n  add_recipe(lat_lon_recipe) %>% \r\n  add_model(xgb_spec)\r\n\r\n\r\n\r\nFit on resamples and get performance estimates.\r\n\r\n\r\ndf_train_xgb_folds <- vfold_cv(df_train_xgb, v = 10, strata = y)\r\n\r\nfit_resamples(xgb_wf, resamples = df_train_xgb_folds) %>%\r\n  collect_metrics() %>% \r\n  kable()\r\n\r\n\r\n.metric\r\n.estimator\r\nmean\r\nn\r\nstd_err\r\n.config\r\naccuracy\r\nbinary\r\n0.7927581\r\n10\r\n0.0041516\r\nPreprocessor1_Model1\r\nroc_auc\r\nbinary\r\n0.8749483\r\n10\r\n0.0038339\r\nPreprocessor1_Model1\r\n\r\nPerformance is a bit better than the logistic regression, and especially not bad considering we’re only using two features! Including other features would certainly improve performance, and we could do that while still using this interpretable approach, but for this post I’m going to stick to just latitude and longitude.\r\nLet’s use a LASSO regression to pull out a few of the more important features. Identifying a good cost for the LASSO turns out to be quite tricky: how many leaves do you want? How many is interpretable? How many are needed to maintain performance? It may be necessary to rely on a heuristic here, for example just using the top N leaves.\r\nThe first step, getting predictions from the leaves, looks a little messy, but most of this code is just one-hot encoding the leaves. The {recipes} package can also be used to one-hot data in far fewer lines of code.\r\n\r\n\r\nxgb_fit <- fit(xgb_wf, df_train_xgb)\r\n\r\nadd_leaves <- function(xgb_fit, df) {\r\n  n_trees <- xgb_fit %>% extract_fit_engine() %>% .$niter\r\n  \r\n  predict(xgb_fit, df, type = 'raw', opts = list(predleaf = T)) %>% \r\n    as_tibble(.name_repair = ~ paste0('tree_', 0:(n_trees - 1))) %>% \r\n    mutate(uid = df$uid, .before = tree_1) %>% \r\n    pivot_longer(starts_with('tree')) %>%\r\n    transmute(uid, tree_leaf = paste0(name, '_X', value), value = 1) %>%\r\n    pivot_wider(names_from = tree_leaf, values_from = value, \r\n                values_fill = 0) %>% \r\n    inner_join(df, by = 'uid')\r\n}\r\n\r\ndf_split$data <- add_leaves(xgb_fit, df_split$data)\r\ndf_train_lr <- add_leaves(xgb_fit, df_train_lr)\r\ndf_test <- add_leaves(xgb_fit, df_test)\r\n\r\n\r\n\r\nNow we can fit a linear model, either on all covariates, or just the leaves. The decision will again depend on the specific goal and domain. In this example, I’m most interested in discovering which combinations of latitude and longitude are most informative, so I will include just those.\r\n\r\n\r\ndf_train_lr_folds <- vfold_cv(df_train_lr, v = 10, strata = y)\r\n\r\nleaf_rec <- recipe(df_train_lr) %>% \r\n  update_role(y, new_role = 'outcome') %>% \r\n  update_role(starts_with('tree_'), new_role = 'predictor') %>% \r\n  step_zv(all_predictors())\r\n\r\nleaf_spec <- logistic_reg(mode = 'classification', engine = 'glmnet',\r\n                          mixture = 1, penalty = 0.1)\r\n\r\nleaf_wf <- workflow() %>% \r\n  add_recipe(leaf_rec) %>% \r\n  add_model(leaf_spec)\r\n\r\n\r\n\r\nLet’s fit this LASSO on some cross-folds and get a performance estimate. The penalty here is relatively harsh sp we should expect the performance to be slightly worse than the XGBoost model. This is fine though, remember we just want to pull out the top embeddings to use in our final linear model.\r\n\r\n\r\nleaf_cv <- fit_resamples(leaf_wf, resamples = df_train_lr_folds)\r\n\r\nleaf_cv %>% \r\n  collect_metrics() %>% \r\n  kable()\r\n\r\n\r\n.metric\r\n.estimator\r\nmean\r\nn\r\nstd_err\r\n.config\r\naccuracy\r\nbinary\r\n0.7442199\r\n10\r\n0.0155847\r\nPreprocessor1_Model1\r\nroc_auc\r\nbinary\r\n0.8409025\r\n10\r\n0.0148693\r\nPreprocessor1_Model1\r\n\r\nAs expected, a bit worse, but still not terrible considering we’re using two base features and regularizing out many of the XGBoost model’s trees. Let’s figure out which trees represent the best embeddings. Rather than using a single fit for this, let’s look at which leaves show up as important across cross-validation rounds. Note that the scoring mechanism used here is purely speculative, I hope that this will result in a more robust selection of embeddings, but this could be an interesting place to explore options.\r\n\r\n\r\nleaf_importance <- map_dfr(df_train_lr_folds$splits, ~ {\r\n  fit(leaf_wf, analysis(.x)) %>% \r\n    extract_fit_engine() %>% \r\n    vip::vi() %>% \r\n    filter(Importance >= 1)\r\n}) %>% \r\n  group_by(Variable) %>% \r\n  summarise(mean = mean(Importance), \r\n            sd = sd(Importance),\r\n            n = n(),\r\n            score = n*((mean)/sd)) %>% \r\n  ungroup() %>% \r\n  filter(n >= 5) %>% \r\n  arrange(desc(score), Variable)\r\n\r\nhead(leaf_importance) %>% \r\n  kable()\r\n\r\n\r\nVariable\r\nmean\r\nsd\r\nn\r\nscore\r\ntree_58_X3\r\n2.519602\r\n0.2440208\r\n10\r\n103.25358\r\ntree_95_X4\r\n1.722802\r\n0.2680110\r\n10\r\n64.28102\r\ntree_97_X4\r\n6.440888\r\n1.0307758\r\n10\r\n62.48584\r\ntree_48_X3\r\n11.820779\r\n1.8933194\r\n10\r\n62.43415\r\ntree_57_X6\r\n4.634435\r\n0.7443771\r\n10\r\n62.25924\r\ntree_89_X5\r\n3.656289\r\n0.7083346\r\n10\r\n51.61811\r\n\r\nWith the key leaves selected, we can move on to a final linear model that uses both these leaves and the other features available.\r\nFinal Linear Model\r\nSpecify and fit the final model.\r\n\r\n\r\nbest_leaves <- leaf_importance %>% \r\n  top_n(n = 10, wt = score) %>% \r\n  pull(Variable)\r\n\r\nflr_recipe <- recipe(df_train_lr) %>% \r\n  update_role(y, new_role = 'outcome') %>%\r\n  update_role(hasSpa, garageSpaces, yearBuilt,\r\n              numOfPatioAndPorchFeatures, lotSizeSqFt,\r\n              avgSchoolRating, MedianStudentsPerTeacher,\r\n              numOfBathrooms, numOfBedrooms,\r\n              new_role = 'predictor') %>% \r\n  update_role(all_of(best_leaves), new_role = 'predictor')\r\n\r\nflr_wf <- workflow() %>% \r\n  add_recipe(flr_recipe) %>% \r\n  add_model(lr_spec)\r\n\r\n\r\n\r\nLet’s get final performance estimates and see if all of this was worth it.\r\n\r\n\r\nfit_resamples(flr_wf, resamples = df_train_lr_folds) %>%\r\n  collect_metrics() %>% \r\n  kable()\r\n\r\n\r\n.metric\r\n.estimator\r\nmean\r\nn\r\nstd_err\r\n.config\r\naccuracy\r\nbinary\r\n0.8009759\r\n10\r\n0.0086919\r\nPreprocessor1_Model1\r\nroc_auc\r\nbinary\r\n0.8768469\r\n10\r\n0.0126593\r\nPreprocessor1_Model1\r\n\r\nThe result is a pretty decent performance bump! So far we’ve been totally ignoring test performance, let’s take a look at that as well, across all of the models.\r\n\r\n\r\nbind_rows(\r\n  last_fit(lr_wf, df_split, metrics = mc_metrics) %>% \r\n    collect_metrics() %>% \r\n    transmute(model = 'logistic_regression', .metric, .estimate),\r\n  last_fit(xgb_wf, df_split, metrics = mc_metrics) %>% \r\n    collect_metrics() %>% \r\n    transmute(model = 'xgboost_lat_lon', .metric, .estimate),\r\n  last_fit(leaf_wf, df_split, metrics = mc_metrics) %>% \r\n    collect_metrics() %>% \r\n    transmute(model = 'lasso_leaf', .metric, .estimate),  \r\n  last_fit(flr_wf, df_split, metrics = mc_metrics) %>% \r\n    collect_metrics() %>% \r\n    transmute(model = 'final_logistic_regression', .metric, .estimate)\r\n) %>% \r\n  pivot_wider(names_from = .metric, values_from = .estimate) %>% \r\n  kable()\r\n\r\n\r\nmodel\r\naccuracy\r\nroc_auc\r\nlogistic_regression\r\n0.7824\r\n0.8519190\r\nxgboost_lat_lon\r\n0.8080\r\n0.8897721\r\nlasso_leaf\r\n0.7548\r\n0.8500604\r\nfinal_logistic_regression\r\n0.8166\r\n0.8904807\r\n\r\nIt’s not amazing, but that’s a nice performance bump. I’m positive a bit of fine-tuning (and better selection of leaves) could result in improved performance.\r\nInterpreting the Leaves\r\nSo far, we’ve acted like just having the embeddings in a linear model means that this model is interpretable. But what are the leaves actually representing? Fortunately, we fit XGBoost with fairly shallow trees, so it should be pretty easy to actually inspect them!\r\nHere’s a function that can navigate a tree and summarize each leaf.\r\n\r\n\r\nsummarize_leaves <- function(tree, id, sofar) {\r\n  row <- tree %>% filter(ID == id)\r\n  \r\n  if (row$Feature == 'Leaf') {\r\n    ret <- list()\r\n    ret[[row$ID]] <- sofar\r\n    return(ret)\r\n  } else {\r\n    left_sofar = c(sofar, glue(\"{row$Feature}<{row$Split}\"))\r\n    left_tree = summarize_leaves(tree, row$Yes, left_sofar)\r\n    \r\n    right_sofar = c(sofar, glue(\"{row$Feature}>={row$Split}\"))\r\n    right_tree = summarize_leaves(tree, row$No, right_sofar)\r\n    \r\n    return(c(left_tree, right_tree))\r\n  }\r\n}\r\n\r\n\r\n\r\nLet’s get the XGBoost tree and then extract the leaves of interest.\r\n\r\n\r\nxgb_tree <- xgboost::xgb.model.dt.tree(model = xgb_fit %>% extract_fit_engine())\r\n\r\ntrees <- best_leaves %>% \r\n  str_extract('^tree_[0-9]*') %>% \r\n  str_remove('^tree_')\r\nleaves <- best_leaves %>% \r\n  str_extract('X[0-9]*') %>%\r\n  str_remove('^X')\r\n\r\nbest_embeddings <- map_dfr(1:length(trees), function(index) {\r\n  toi <- trees[index]\r\n  loi <- leaves[index]\r\n  leaf_label <- glue('{toi}-{loi}')\r\n  tibble(\r\n    tree = toi,\r\n    leaf = loi,\r\n    label = leaf_label,\r\n    emb = xgb_tree %>% \r\n      filter(Tree == toi) %>% \r\n      summarize_leaves(glue('{toi}-0'), c()) %>% \r\n      pluck(leaf_label)\r\n  )\r\n})\r\n\r\nhead(best_embeddings) %>% \r\n  kable()\r\n\r\n\r\ntree\r\nleaf\r\nlabel\r\nemb\r\n58\r\n3\r\n58-3\r\nlatitude<30.4451027\r\n58\r\n3\r\n58-3\r\nlongitude<-97.6950073\r\n95\r\n4\r\n95-4\r\nlatitude<30.4164162\r\n95\r\n4\r\n95-4\r\nlatitude>=30.3882008\r\n97\r\n4\r\n97-4\r\nlatitude<30.4989376\r\n97\r\n4\r\n97-4\r\nlatitude>=30.4512882\r\n\r\nLet’s take this just one step further and plot these on top of our data! We’ll need to do a little processing of the embeddings to plot them in a palatable way. Specifically, the max/min of the lat/lon (depending on the direction of the cutoff) is what ultimately determines how the embedding works. As such, these are what I’ll focus on plotting.\r\n\r\n\r\nbest_embeddings <- best_embeddings %>% \r\n  mutate(direction = ifelse(str_detect(emb, \">=\"), \">=\", \"<\")) %>% \r\n  separate(emb, c('variable', 'cutpoint'), \"(\\\\>\\\\=|\\\\<)\", convert = T) %>% \r\n  distinct() %>% \r\n  group_by(label, variable, direction) %>% \r\n  filter(case_when(\r\n    direction == \">=\" ~ cutpoint == max(cutpoint),\r\n    direction == \"<\" ~ cutpoint == min(cutpoint),\r\n  ))\r\n\r\nggplot(data = best_embeddings) +\r\n  geom_point(data = df, aes(latitude, longitude, colour = y),\r\n             alpha = 0.1) +\r\n  geom_vline(data = . %>% filter(variable == 'latitude'),\r\n             aes(xintercept = cutpoint, colour = direction),\r\n             size = 1) +\r\n  geom_hline(data = . %>% filter(variable == 'longitude'),\r\n             aes(yintercept = cutpoint, colour = direction),\r\n             size = 1) +\r\n  facet_wrap(vars(label), ncol = 5) +\r\n  scale_color_few() +\r\n  labs(colour = '') +\r\n  theme_minimal() +\r\n  theme(legend.position = 'bottom',\r\n        axis.text = element_blank())\r\n\r\n\r\n\r\n\r\nFrom this we can see the regions that our data is identifying. They’re clearly not perfect, but they definitely do identify some regional cutoffs. How many of these embeddings apply to each user?\r\n\r\n\r\ndf_train_lr %>% \r\n  select(uid, best_leaves) %>% \r\n  pivot_longer(starts_with('tree')) %>% \r\n  group_by(uid) %>% \r\n  summarise(number_of_active_embeddings = sum(value)) %>% \r\n  ungroup() %>% \r\n  count(number_of_active_embeddings) %>% \r\n  kable()\r\n\r\n\r\nnumber_of_active_embeddings\r\nn\r\n0\r\n117\r\n1\r\n428\r\n2\r\n223\r\n3\r\n210\r\n4\r\n216\r\n5\r\n57\r\n\r\nGiven that the I’m using very simple trees, this makes a lot of sense to me. I’d argue that these are still interpretable. Checking the number of leaves applying to a given observation may also be a good way to check for overfitting.\r\nThis concludes my example. I think a lot more optimization could be done to improve this model, but I’m quite happy with it as a first pass.\r\nRelated Ideas\r\nThis post is already long, at least by my standards, but I just want to hit on a few other very neat things this approach allows.\r\nMissing Data\r\nXGBoost can still be used with missing data, the algorithm simply lumps all observations with missing for the selected variable into one side of the decision tree. As such, this approach could be used to create embeddings of columns with missing data, the resulting embeddings can then be used in models which do not typically support missing data.\r\nOther Tree Based Models\r\nThere’s nothing special about XGBoost, any tree-based model could be used in its place. I suspect, this could be advantageous in some situations.\r\nTuning\r\nRather than tuning the XGBoost hyperparameters and then using the leaves in a subsequent model, we could bundle all steps into a hyperparameter tuning loop. This would allow us to tune XGBoost specifically to create good embeddings. However, I’m not sure if this will result in practically better performance.\r\nConclusion\r\nWhile not perfect, the use of a tree-based model to create features that are interpretable and usable in linear models is something I find really exciting. Specifically because this approach can combine the best-in-class performance of decision forests with the interpretability of linear models that is often required in regulated environments. This is doubtless an area I will continue to explore.\r\nPlease feel free to reach out with any questions or corrections!\r\n\r\n\r\n\r\nHead, Tim. 2021. “Feature Transformations with Ensembles of Trees.” https://scikit-learn.org/stable/auto_examples/ensemble/plot_feature_transformation.html.\r\n\r\n\r\nLoft. 2021. “Xgbse: XGBoost Survival Embeddings.” https://loft-br.github.io/xgboost-survival-embeddings/index.html.\r\n\r\n\r\nRudin, Cynthia. 2019. “Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.” https://arxiv.org/abs/1811.10154.\r\n\r\n\r\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. “Fooling LIME and SHAP: Adversarial Attacks on Post Hoc Explanation Methods.” https://arxiv.org/abs/1911.02508.\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2022-01-31-xgboost-as-a-feature-generator/xgboost-as-a-feature-generator_files/figure-html5/cool-plot-1.png",
    "last_modified": "2022-02-27T13:51:31-08:00",
    "input_file": "xgboost-as-a-feature-generator.knit.md",
    "preview_width": 2304,
    "preview_height": 1536
  },
  {
    "path": "posts/2020-03-16-what-is-an-improper-prior/",
    "title": "What is an improper prior?",
    "description": "A short introduction to the use of improper priors in Bayesian statistics.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-03-16",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMathJax.Hub.Config({\r\n  TeX: { \r\n      equationNumbers: { \r\n            autoNumber: \"AMS\"\r\n      } \r\n  }\r\n});\r\nIntroduction\r\nWhile I’ve purchased improperprior.com as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.\r\nThis post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.\r\nPriors As Usual\r\nIn the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you’re into Empirical Bayes), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.\r\nThis process is powered by Bayes’ theorem. For example, suppose we have data \\(Y\\) that follow a Binomial distribution, and we would like to estimate the \\(\\theta\\) parameter. Using Bayes’ theorem we would compute:\r\n\\[\\begin{equation}\r\n\\tag{1}\r\nP(\\theta|Y) = \\frac{L(Y|\\theta)P(\\theta)}{P(Y)}.\r\n\\end{equation}\\]\r\nWhere:\r\n\\(P(\\theta|Y)\\) is the posterior probability, this is what we’d like to estimate.\r\n\\(L(Y|\\theta)\\) is the likelihood of observing the data, given \\(\\theta\\).\r\n\\(P(\\theta)\\) is a distribution representing a prior guess for \\(\\theta\\) before observing data.\r\n\\(P(Y)\\) is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a proper distribution), which is surprisingly unnecessary for posterior estimation.\r\nThe canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it’s potential shapes.\r\n\r\n\r\n\r\nFor this example we’ll use a flat prior, which gives equal weight to all possible values of \\(\\theta\\). The \\(\\text{Beta}(1, 1)\\) distribution does this by just giving a Uniform distribution over \\([0, 1]\\).\r\nThe likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just \\(\\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i}\\). We can complete the numerator of (1) by combining this likelihood with the \\(\\text{Beta}(1, 1)\\) prior, which gives:\r\n\\[\\begin{equation}\r\n\\tag{2}\r\nP(\\theta|Y) \\propto \\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i} \\times \\theta^{1-1}(1-\\theta)^{1-1} \\\\\r\n            \\propto \\theta^{\\sum_i y_i + 1 - 1}(1-\\theta)^{n-\\sum_i y_i  + 1 - 1}.\r\n\\end{equation}\\]\r\nFrom this we can see that the posterior distribution follows a \\(\\text{Beta}(\\sum_i y_i + 1, n - \\sum_i y_i + 1)\\) distribution. Intuitively because we used a prior that carried little information with it (effectively saying that \\(\\theta\\) could be any value in (0, 1)) our posterior estimate for \\(\\theta\\) is entirely determined by the data.\r\nHere’s a simple Shiny app to let you play with this model and build some intuition as to how the prior parameters and data interact.\r\n\r\n\r\n\r\n\r\nLimitations\r\nSo far this seems like a great framework, but the requirement that the prior be a proper distribution can be quite restrictive. Consider the normal distribution (for simplicity, assume known \\(\\sigma^2\\)): \\(\\mu\\) can take any real value, so how can we use a flat prior with equal probability for each possible value of \\(\\mu\\)?\r\n\r\nNote that a proper distribution is one with a density function that integrates to 1.\r\nWhile powerful in specific cases, Bayesian modeling is rather limited if we can only use proper distributions as priors.\r\nImproper Priors\r\nNaively, what would happen if we just set the probability of each \\(\\mu\\) to 1? Well it turns out that we can do exactly this - we can use any prior, even an improper prior as long as the posterior comes out to be a proper distribution.\r\nChoosing an improper prior that generates a valid posterior can be a tricky affair, but using Jeffreys’ prior is a good place to start. Continuing the normal example, we will just use a prior probability of 1 for every value of \\(\\mu\\). This is actially proportional to the Jeffreys’ prior for this setup. As with the previous example, we will set aside all constant terms:\r\n\\[\r\nP(\\theta|Y) \\propto \\exp{\\left[-\\frac{1}{2} \\left(\\frac{y-\\mu}{\\sigma^2}\\right)^2\\right]} \\times 1.\r\n\\]\r\nThe posterior is just a proper normal distribution!\r\nIt is of course possible to go much deeper into improper priors, particularly choosing a good prior, but as far as the concept goes, this is mostly all there is to it!\r\nFurther Resources\r\nCraig Gidney has a nice blog post walking through a slightly more technical example of improper priors. Likewise, Andy Jones has a great podcast with a few additional examples. For a more general treatment A First Course in Bayesian Statistical Methods by Hoff and Bayesian Data Analysis by Gelman et al are the standard introductory Bayesian statistics textbooks.\r\nAs ever, Wikipedia has very detailed articles on priors, more suitable for reference than learning:\r\nImproper Priors\r\nJeffreys’ prior\r\nConjugate Priors\r\nFurther Resources\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-08-20T08:21:32-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-02-14-continuous-and-binary-dependent-variables/",
    "title": "Continuous and Binary Dependent Variables",
    "description": "An exploration of the use of continuous and binary dependent variables in causal inference.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-02-14",
    "categories": [],
    "contents": "\r\nIntroduction\r\nIn his 2011 paper “Land tenure and investment incentives: Evidence from West Africa,” James Fenske notes that “studies that use binary investment measures…are also less likely to find a statistically significant effect.” He seems to attribute this to (1) small sample sizes, and (2) the lack of nuance associated with binary variables. Fenske argues that continuous measures of investment intensity are better for causal analysis. As Fenske points out continuous variables are usually noisy. For example, imagine asking a farmer how many KGs of fertilizer they applied to each plot of land in the past year versus asking a farmer if they applied any fertilizer to each plot of land in the past year.\r\nWhile small sample sizes can certainly cause estimation issues (frequentist statistics rely heavily on asymptotic results afterall), I’d like to explore why binary variables may be showing up more often as statistically insignificant in this context.\r\nI will be assuming the use of OLS rather than logistic regression, which is somewhat common in the economics literature for binary variables. I will write in terms of fertilizer amount (continuous) /fertilizer use (binary) and a treatment meant to increase fertilizer use, but these results generalize.\r\nR Setup\r\n\r\n\r\n# Data manipulation\r\nlibrary(dplyr)\r\nlibrary(purrr)\r\nlibrary(tidyr)\r\n# Modeling\r\nlibrary(broom)\r\n# Plotting and tables\r\nlibrary(ggplot2)\r\nlibrary(ggcute)\r\nlibrary(knitr)\r\n\r\n# R settings\r\ntheme_set(theme_minimal() + theme(legend.position = \"none\"))\r\ntheme_update(panel.background = element_rect(fill = \"transparent\", \r\n                                             colour = NA),\r\n             plot.background = element_rect(fill = \"transparent\", \r\n                                            colour = NA))\r\n\r\nopts_chunk$set(echo = T, warning = F, message = F, tidy = F,\r\n               fig.width = 8.5, fig.height = 6,\r\n               dev.args = list(bg = \"transparent\"))\r\n\r\n\r\n\r\nInformation Loss\r\nBinarizing a continuous variable, even if noisy, will result in some amount of information loss. As Fenske points out, we go from estimating the amount of fertilizer used to an indicator for any fertilizer use. If few farmers use fertilizer to begin with, then information loss will be low. That is, we can still tell the difference between farmers who fertilize, and those who don’t. However, if most farmers use some amount of fertilizer, then the binarization will make it very difficult to test for any treatment effect.\r\n\r\n\r\n# Simulate information loss\r\nbind_rows(\r\n  tibble(\r\n    y = rpois(1000, 0.5),\r\n    y_bin = ifelse(y > 0, 1, 0),\r\n    lab = \"Low Information Loss\"\r\n  ),\r\n  tibble(\r\n    y = rpois(1000, 2),\r\n    y_bin = ifelse(y > 0, 1, 0),\r\n    lab = \"High Information Loss\"\r\n  )\r\n) %>% \r\n  gather(variable, value, -lab) %>% \r\n  ggplot(aes(value, fill = variable)) +\r\n  geom_bar(position = \"dodge\") +\r\n  facet_wrap(vars(lab)) +\r\n  scale_fill_fairyfloss()\r\n\r\n\r\n\r\n\r\nSimulations\r\nI will be using simulations to explore this issue. Data will be drawn from a Poisson distribution, which has mean equal to its only parameter, lambda. This is quite convenient for simulating treatment effects, as we can easily increase the treatment effect by shifting lambda.\r\n\r\n\r\nsim_settings <- expand_grid(\r\n  # Number of repeats for each group of simulation settings\r\n  m = 1:5,\r\n  # Sample size\r\n  n = seq(100, 3000, 100),\r\n  # Lambda (mean of Poisson)\r\n  lambda = c(0.5, 1, 1.5),\r\n  # Treatment effect in absolute terms\r\n  # (i.e. added to lambda)\r\n  treat_effect = seq(0.01, 0.2, 0.05)\r\n)\r\nsims <- pmap_df(sim_settings, function(m, n, lambda, treat_effect) {\r\n  # Generate data\r\n  treat <- rep(0:1, each = n/2)\r\n  y <- c(rpois(n/2, lambda), rpois(n/2, lambda + treat_effect))\r\n  y_bin <- ifelse(y > 0, 1, 0)\r\n  \r\n  # Run models\r\n  bind_rows(\r\n    lm(y ~ treat) %>% tidy(),\r\n    lm(y_bin ~ treat) %>% tidy()\r\n  ) %>% \r\n    filter(term == \"treat\") %>% \r\n    mutate(m = m, n = n, lambda = lambda, treat_effect = treat_effect,\r\n           outcome = c(\"continuous\", \"binary\"))\r\n})\r\n\r\n\r\n\r\nBinarization will cause issues estimating a precise treatment effect, with higher information loss (i.e. higher lambda) leading to more bias.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(bias = mean(estimate - treat_effect)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, bias, colour = outcome)) +\r\n  geom_hline(yintercept = 0.0) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_fairyfloss() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nWhile effect size estimates are biased, they still seem to work well enough for significance testing in situations where information loss isn’t extreme.\r\n\r\n\r\nsims %>% \r\n  group_by(n, lambda, treat_effect, outcome) %>% \r\n  summarise(p.value = mean(p.value)) %>% \r\n  ungroup() %>% \r\n  mutate(lambda = paste0(\"Lambda = \", lambda),\r\n         treat_effect = paste0(\"Treat = \", treat_effect)) %>% \r\n  ggplot(aes(n, p.value, colour = outcome)) +\r\n  geom_hline(yintercept = 0.1) +\r\n  geom_point() +\r\n  geom_line() +\r\n  scale_colour_fairyfloss() +\r\n  facet_grid(treat_effect ~ lambda) +\r\n  theme(legend.position = \"bottom\") +\r\n  labs(title = \"Sample Size Vs. Bias\",\r\n       subtitle = \"across Treatment Effect Size and Lambda\")\r\n\r\n\r\n\r\n\r\nConclusion\r\nI suspect Fenske is correct to question the use of binary investment indicators over continuous intensivity indicators. However, it is clear to me that binary indicators are fine for many situations, e.g. measuring the up-take of new agricultural technologies. For situations where the outcome is already common at a low level (and the sample size is high enough), a coarse grained intensivity measure may provide enough information to capture a reasonable estimate of the treatment effect.\r\nSession Info\r\n\r\n\r\nsessionInfo()\r\n\r\n\r\nR version 4.1.0 (2021-05-18)\r\nPlatform: x86_64-w64-mingw32/x64 (64-bit)\r\nRunning under: Windows 10 x64 (build 19042)\r\n\r\nMatrix products: default\r\n\r\nlocale:\r\n[1] LC_COLLATE=English_United States.1252 \r\n[2] LC_CTYPE=English_United States.1252   \r\n[3] LC_MONETARY=English_United States.1252\r\n[4] LC_NUMERIC=C                          \r\n[5] LC_TIME=English_United States.1252    \r\n\r\nattached base packages:\r\n[1] stats     graphics  grDevices utils     datasets  methods  \r\n[7] base     \r\n\r\nother attached packages:\r\n[1] knitr_1.33        ggcute_0.0.0.9000 ggplot2_3.3.5    \r\n[4] broom_0.7.8       tidyr_1.1.3       purrr_0.3.4      \r\n[7] dplyr_1.0.7      \r\n\r\nloaded via a namespace (and not attached):\r\n [1] highr_0.9         pillar_1.6.1      bslib_0.2.5.1    \r\n [4] compiler_4.1.0    jquerylib_0.1.4   tools_4.1.0      \r\n [7] digest_0.6.27     downlit_0.2.1     gtable_0.3.0     \r\n[10] jsonlite_1.7.2    evaluate_0.14     lifecycle_1.0.0  \r\n[13] tibble_3.1.2      pkgconfig_2.0.3   rlang_0.4.11     \r\n[16] DBI_1.1.1         distill_1.2       yaml_2.2.1       \r\n[19] xfun_0.24         withr_2.4.2       stringr_1.4.0    \r\n[22] generics_0.1.0    vctrs_0.3.8       sass_0.4.0       \r\n[25] grid_4.1.0        tidyselect_1.1.1  glue_1.4.2       \r\n[28] R6_2.5.0          fansi_0.5.0       rmarkdown_2.9    \r\n[31] farver_2.1.0      magrittr_2.0.1    scales_1.1.1     \r\n[34] backports_1.2.1   ellipsis_0.3.2    htmltools_0.5.1.1\r\n[37] assertthat_0.2.1  colorspace_2.0-2  labeling_0.4.2   \r\n[40] utf8_1.2.1        stringi_1.6.2     munsell_0.5.0    \r\n[43] crayon_1.4.1     \r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-02-14-continuous-and-binary-dependent-variables/continuous-and-binary-dependent-variables_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-08-20T08:39:22-07:00",
    "input_file": {},
    "preview_width": 1632,
    "preview_height": 1152
  }
]
