[
  {
    "path": "posts/2020-03-28-what-is-an-improper-prior/",
    "title": "What is an improper prior?",
    "description": "A short description of the post.",
    "author": [
      {
        "name": "Ben Ewing",
        "url": "https://improperprior.com/"
      }
    ],
    "date": "2020-03-28",
    "categories": [],
    "contents": "\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nMathJax.Hub.Config({\r\n  TeX: { \r\n      equationNumbers: { \r\n            autoNumber: \"AMS\"\r\n      } \r\n  }\r\n});\r\nIntroduction\r\nWhile I’ve purchased improperprior.com as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.\r\nThis post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.\r\nPriors As Usual\r\nIn the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you’re into Empirical Bayes), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.\r\nThis process is powered by Bayes’ theorem. For example, suppose we have data \\(Y\\) that follow a Binomial distribution, and we would like to estimate the \\(\\theta\\) parameter. Using Bayes’ theorem we would compute:\r\n\\[\\begin{equation}\r\n\\tag{1}\r\nP(\\theta|Y) = \\frac{L(Y|\\theta)P(\\theta)}{P(Y)}.\r\n\\end{equation}\\]\r\nWhere:\r\n\\(P(\\theta|Y)\\) is the posterior probability, this is what we’d like to estimate.\r\n\\(L(Y|\\theta)\\) is the likelihood of observing the data, given \\(\\theta\\).\r\n\\(P(\\theta)\\) is a distribution representing a prior guess for \\(\\theta\\) before observing data.\r\n\\(P(Y)\\) is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a proper distribution), which is surprisingly unnecessary for posterior estimation.\r\nThe canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it’s potential shapes. For this example we’ll use a flat prior, which gives equal weight to all possible values of \\(\\theta\\). The \\(\\text{Beta}(1, 1)\\) distribution does this by just giving a Uniform distribution over \\([0, 1]\\).\r\n\r\n\r\n\r\nThe likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just \\(\\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i}\\). We can complete the numerator of (1) by combining this likelihood with the \\(\\text{Beta}(1, 1)\\) prior, which gives:\r\n\\[\\begin{equation}\r\n\\tag{2}\r\nP(\\theta|Y) \\propto \\theta^{\\sum_i y_i}(1-\\theta)^{n-\\sum_i y_i} \\times \\theta^{1-1}(1-\\theta)^{1-1} \\\\\r\n            \\propto \\theta^{\\sum_i y_i + 1 - 1}(1-\\theta)^{n-\\sum_i y_i  + 1 - 1}.\r\n\\end{equation}\\]\r\nLimitations\r\nImproper Priors\r\nFurther Resources\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-07-27T08:14:09-07:00",
    "input_file": "what-is-an-improper-prior.knit.md"
  }
]
