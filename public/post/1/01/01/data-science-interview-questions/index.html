<!DOCTYPE html>
<html lang="en-us">
  <head>
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
    <link rel="manifest" href="/images/site.webmanifest">

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="Ben Ewing&#39;s blog.">
    <title>Data Science Interview Questions | Improper Prior</title>
    <link rel="stylesheet" href="/css/style.css" />
    <link rel="stylesheet" href="/css/fonts.css" />
    <link rel="stylesheet" href="/css/theme-override.css">
    <header>

  <nav>
    <ul>
      
      
      <li class="pull-left ">
        <a href="/">improper_prior</a>
      </li>
      
      
      <li class="pull-left ">
        <a href="/friends/">friends_on_the_internet</a>
      </li>
      

      

    </ul>
  </nav>
</header>

  </head>

  <body>
    <br/>

<div class="article-meta">
<h1><span class="title">Data Science Interview Questions</span></h1>
<h2 class="author">Ben Ewing</h2>

<p class="terms">
  
  
  Categories: <a href="/categories/technical">technical</a> 
  
  
  
  
</p>
</div>


<div class="content-wrapper">
  <main>
    


<div id="introduction" class="section level2">
<h2>Introduction</h2>
<p>I‚Äôve seen a number of articles/posts pop up around the internet with lists of data science interview questions. I thought it might be a good exercise in communication and data science knowledge to answer these questions. I am finding it harder than I would have expected to write good (and hopefully useful) answers!</p>
<p>Question sources:</p>
<ul>
<li><a href="https://hackernoon.com/160-data-science-interview-questions-415s3y2a">HackerNoon - 160+ Data Science Interview Questions</a></li>
</ul>
<p>Additional resources:</p>
<ul>
<li><a href="http://alexeygrigorev.com/">Alexey Grigorev</a> has put together a whole GitHub repo with data science interview questions <em>and</em> answers, <a href="https://github.com/alexeygrigorev/data-science-interviews/">available here</a>. He also has his own extensive collection of <a href="https://github.com/alexeygrigorev/data-science-interviews/blob/master/awesome.md">resources</a>.</li>
<li><a href="https://chrisalbon.com/">Chris Albon</a> has a collection of machine learning flashcards, <a href="https://machinelearningflashcards.com/">available as a whole set for a price</a>, or sporadically posted <a href="https://twitter.com/chrisalbon">on his Twitter feed</a>. I‚Äôve found these to be a useful way to identify gaps in my knowledge.</li>
</ul>
</div>
<div id="supervised-machine-learning" class="section level2">
<h2>Supervised machine learning</h2>
<div id="what-is-supervised-machine-learning" class="section level3">
<h3>What is supervised machine learning?</h3>
<p>Supervised machine learning is when we use a dataset, <span class="math inline">\(X\)</span>, to predict some known target value <span class="math inline">\(y\)</span> where <span class="math inline">\(y\)</span> might be a label/class or some continuous value. In unsupervised learning we learn about a dataset <span class="math inline">\(X\)</span> without any label, for example clustering individuals.</p>
</div>
</div>
<div id="linear-regression" class="section level2">
<h2>Linear Regression</h2>
<div id="what-is-regression-which-models-can-you-use-to-solve-a-regression-problem" class="section level3">
<h3>What is regression? Which models can you use to solve a regression problem?</h3>
<p>In a regression problem we predict a continuous value (rather than a label). Linear regression is very common; it can be augmented with polynomial features or regularization terms (lasso and ridge regression). Many other algorithms can also be used for regression problems, such as random forests with regression trees.</p>
</div>
<div id="what-is-linear-regression-when-do-we-use-it" class="section level3">
<h3>What is linear regression? When do we use it?</h3>
<p>For data <span class="math inline">\(X\)</span>, observation <span class="math inline">\(i\)</span>, a linear regression predicts a value <span class="math inline">\(y_i = x_i^T\beta\)</span> for a set of coefficients <span class="math inline">\(\beta\)</span>. We typically assume there to be some error term <span class="math inline">\(\epsilon_i\)</span> that is normally distributed around 0.</p>
<p>Additional notes:</p>
<ul>
<li><p>We can add an extra column of 1s to the data to include an intercept term. The column of 1s allows us to add an extra <span class="math inline">\(\beta_0\)</span> which (in the case of linear regression with a single variable) changes where on the y-axis the line passes through. Without this term the line will just pass through 0.</p></li>
<li><p>The ‚Äòlinear‚Äô in linear regression refers to the relationship between <span class="math inline">\(y\)</span> and <span class="math inline">\(\beta\)</span>, the <span class="math inline">\(x_i\)</span>‚Äôs can be polynomials, as in the case of polynomial regression.</p></li>
<li><p>While the <span class="math inline">\(\beta\)</span>‚Äôs can be found through a number of techniques, choosing <span class="math inline">\(\beta\)</span> to minimize <span class="math inline">\((y- X\beta)^T(y- X\beta)\)</span> (as is very common, this is just ordinary least squares) gives a very elegant closed form: <span class="math inline">\(\hat{\beta} = (X^TX)^{-1}X^Ty\)</span>.</p></li>
</ul>
</div>
<div id="whats-the-normal-distribution-why-do-we-care-about-it" class="section level3">
<h3>What‚Äôs the normal distribution? Why do we care about it?</h3>
<p>The normal distribution with with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> as <span class="math inline">\(p(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left(-\frac{1}{2\sigma^2}(x-\mu)^2\right)}\)</span>.</p>
<p>There are a lot of reasons to care about the normal distribution (e.g.¬†Alexey talks about the central limit theorem for this question). Perhaps the reason the normal distribution pops up everywhere is that it‚Äôs kernel (with known variance) is just the squared difference between two quantities, which we use quite often.</p>
</div>
<div id="how-do-we-check-if-a-variable-follows-the-normal-distribution" class="section level3">
<h3>How do we check if a variable follows the normal distribution?</h3>
<p>For exploratory data analysis (EDA) I would just plot a histogram or the empirical density. For a more formal setting one could look at the skew and kurtosis (both 0 for a normal distribution), or run any one of the many <a href="https://en.wikipedia.org/wiki/Normality_test">normality tests</a>.</p>
<p>This is a pretty common procedure when looking at regression residuals, remember we want a normal error!</p>
</div>
<div id="what-if-we-want-to-build-a-model-for-predicting-prices-are-prices-distributed-normally-do-we-need-to-do-any-pre-processing-for-prices" class="section level3">
<h3>What if we want to build a model for predicting prices? Are prices distributed normally? Do we need to do any pre-processing for prices?</h3>
<p>My best guess (i.e.¬†based on only my priors, no data) is that prices are not normally distributed, in fact I would expect the distribution of prices to be very skewed due to the presence of high-end luxury goods. We can deal with this by using a log-transformation on the data. I assume this question is not asking about pre-processing like converting to the same currency or other data cleaning steps.</p>
</div>
<div id="what-are-the-methods-for-solving-linear-regression-do-you-know" class="section level3">
<h3>What are the methods for solving linear regression do you know?</h3>
<p>See my answer for ‚ÄúWhat is linear regression? When do we use it?‚Äù for the closed form approach (and where it comes from), but note that we can also use any(?) gradient-based optimization approach as well.</p>
</div>
</div>
<div id="what-is-gradient-descent-how-does-it-work" class="section level2">
<h2>What is gradient descent? How does it work?</h2>
<p>Gradient descent a calculus-based optimizatin method that iteratively updates parameters in a model in the opposite direction of the gradient. For a linear regression model gradient descent looks something like this: <span class="math inline">\(\mathbf{\beta}_{i+1} = \mathbf{\beta}_i -\alpha\nabla_\beta(\mathbf{X}^T\mathbf{\beta}_i - \mathbf{y})^2\)</span> where <span class="math inline">\(\alpha\)</span> is the learning rate (this controls the size of each update).</p>
<p>Note that a gradient is just a multivaraite derivative, so if we have a linear model <span class="math inline">\(\beta_0 + \beta_1x_1\)</span> the gradient with respect to the betas is <span class="math inline">\([1, x]^T\)</span>. <a href="https://ruder.io/optimizing-gradient-descent/index.html">Sebastian Ruder</a> has a really great overview oof gradient-based optimization methods.</p>
<div id="what-is-the-normal-equation" class="section level3">
<h3>What is the normal equation?</h3>
<p>I think this is a silly interview question. A <span class="math inline">\(N(\mu, \sigma^2)\)</span> distribution has PDF <span class="math inline">\(\frac{1}{\sqrt{2\pi\sigma^2}} \exp{\left[-\frac{1}{2\sigma^2}(x-\mu)^2\right]}\)</span>.</p>
</div>
<div id="what-is-sgd-whats-the-difference-with-the-usual-gradient-descent" class="section level3">
<h3>What is SGD? What‚Äôs the difference with the usual gradient descent?</h3>
<p>In ‚Äústandard‚Äù (i.e.¬†Batch) gradient descent we use all of the data at once, but this is computationally infeasible for sufficiently large datasets. Stochastic Gradient Descent randomly shuffles the data and then performs gradient descent one observation at a time to allow for computationally efficient updates. This approach can also be combined with the minibatch approach, where gradient descent is applied to smaller (computationally feasible) subsets of the data.</p>
</div>
<div id="which-metrics-for-evaluating-regression-models-do-you-know" class="section level3">
<h3>Which metrics for evaluating regression models do you know?</h3>
<p>This is also kind of a silly question, what do you want to know about the model? There are goodness-of-fit measures like R-squared, measures of prediction accuracy like MSE, RMSE (next question), and heuristics that can be used to choose from many models like AIC and BIC. There are surely <em>many</em> other ways to evaluate a linear model.</p>
</div>
<div id="what-are-mse-and-rmse" class="section level3">
<h3>What are MSE and RMSE?</h3>
<ul>
<li>Mean Squared Error is the squared difference between a prediction and its true value: <span class="math inline">\(\frac{1}{n}\sum_i^n(y_i - \hat{y}_i)^2\)</span>.</li>
<li>Root Mean Squared Error is the square-root of the MSE. The rescaling just allows us to talk about the error using the same units as the dependent variable, for example:</li>
</ul>
<pre class="r"><code># R

y &lt;- 5
y_hat &lt;- 2

paste0(&quot;MSE: &quot;, mean((y - y_hat)^2), 
       &quot;; RMSE: &quot;, mean((y - y_hat)^2)^0.5)</code></pre>
<pre><code>## [1] &quot;MSE: 9; RMSE: 3&quot;</code></pre>
</div>
</div>
<div id="coding" class="section level2">
<h2>Coding</h2>
<div id="implement-ols-linear-regression" class="section level3">
<h3>Implement OLS Linear Regression</h3>
<p>Some data, note that the outcome really is a linear function of the input! Also note that no intercept is needed.</p>
<pre class="r"><code># R

n &lt;- 1000
x &lt;- matrix(c(rnorm(n), rnorm(n, mean = 1), rnorm(n, sd = 3)),
            nrow = n)
y &lt;- x[ , 1]*0.5 + x[ , 2]*0.25 + x[ , 3]*3</code></pre>
<p>In OLS we assume our outoome is the product of some linear model (as it is in the example data above), and we seek to minimize the squared distance between our predictions, <span class="math inline">\(X^T\beta\)</span>, and the true value <span class="math inline">\(y\)</span>. In matrix form we want to minimze <span class="math inline">\((y- X\beta)^T(y- X\beta)\)</span>. Take the derivative with respect to <span class="math inline">\(\Beta\)</span>, set it equal to 0, solve for <span class="math inline">\(\beta\)</span> (with the assumption that <span class="math inline">\(X\)</span> is invertible), and you end up with <span class="math inline">\(\beta_{OLS} = (X^TX)^{-1}X^Ty\)</span>.</p>
<p>This is an easy closed-form equation to implement in any language, assuming the interviewer doesn‚Äôt require the interviewee to implement their own matrix inversion function.</p>
<pre class="r"><code># R

bens_lm &lt;- function(x, y) {
  solve((t(x) %*% x)) %*% (t(x) %*% y) 
}
bens_lm(x, y)</code></pre>
<pre><code>##      [,1]
## [1,] 0.50
## [2,] 0.25
## [3,] 3.00</code></pre>
<p>Compare with R‚Äôs linear model.</p>
<pre class="r"><code># R

lm(y ~ x)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Coefficients:
## (Intercept)           x1           x2           x3  
##   4.283e-16    5.000e-01    2.500e-01    3.000e+00</code></pre>
</div>
<div id="implement-logistic-regression" class="section level3">
<h3>Implement Logistic Regression</h3>
<p>When using logistic regression we are interested in modeling the probability of an event, usually just a Bernoulli random variable (sidenote: <a href="https://en.wikipedia.org/wiki/Bernoulli_family">the Bernoulli family is pretty incredible</a>). As with linear regression we use a linear model of the form <span class="math inline">\(X^T\beta\)</span> but this time we use a <a href="https://en.wikipedia.org/wiki/Generalized_linear_model#Logit_link_function">logit link function</a> to relate our model to the log odds of the outcome. We can also solve for the probability of the model and get the logistic function; <a href="https://stats.stackexchange.com/questions/120329/what-is-the-difference-between-logistic-and-logit-regression">here</a> is a good StackExchange discussion on what the logit and logistic functions do.</p>
<p>Using the logit link function our model is <span class="math inline">\(\log{\left(\frac{\pi(y = 1|X)}{1-\pi(y = 1|X)}\right)} = X^T\beta\)</span>, which we rewrite using the logistic function as <span class="math inline">\(\pi(y = 1|X) = \frac{1}{1+e^{-X^T\beta}}\)</span>. To fit the model, we want to choose <span class="math inline">\(\beta\)</span> to maximize the likelihood that this model matches our data. The likelihood is just <span class="math inline">\(\prod_i \left(\frac{1}{1+e^{-x_i^T\beta}}\right)^{y_i}\left(1 - \frac{1}{1+e^{-x_i^T\beta}}\right)^{1-y_i}\)</span>, and taking the log gives <span class="math inline">\(\sum_i y_i\log{\left(\frac{1}{1+e^{-x_i^T\beta}}\right)} + (1-y_i)\log{\left(\frac{1}{1+e^{-x_i^T\beta}}\right)}\)</span>. Finally, taking the derivative with respect to <span class="math inline">\(\beta\)</span> gives a pretty intuitive form <span class="math inline">\(X^T\left(y - \frac{1}{1+e^{-x_i^T\beta}}\right)\)</span>. Unfortunately, unlike linear regression, there is no closed form solution, instead we must use an optimization technique to find good values for <span class="math inline">\(\beta\)</span>, we can do this using gradient descent.</p>
<p>For a longer and more detailed walk through I‚Äôd recommend <a href="https://beckernick.github.io/logistic-regression-from-scratch/">Nick Becker‚Äôs post on logistic regression</a>.</p>
<pre class="r"><code># R

# Sigmoid function is useful, this assumes x is a matrix
sigm &lt;- function(x, beta) {
  1/(1 + exp(-apply(x, 1, function(.x) .x %*% beta)))
}

# First, create some data!
n &lt;- 10000
p &lt;- 4
x &lt;- cbind(rep(1, n), replicate(p - 1, rnorm(n)))
true_beta &lt;- rpois(p, 2.5)
y &lt;- rbinom(n, 1, sigm(x, true_beta))

# Now we can begin our search, need to initalize beta, determine number of iterations of gradient
# descent, and choose a learning rate, these are by no means optimal choices.
beta_hat &lt;- rnorm(p, 0, 10)
starting_beta_hat &lt;- beta_hat
n_iters &lt;- 1000
lr &lt;- 1e-3

for (iter in 1:n_iters) {
  # Get predictions using the current weights
  y_hat &lt;- sigm(x, beta_hat)
  
  # Get the gradient
  grad &lt;- c(t(x) %*% (y - y_hat))
  
  # And update the parameters
  beta_hat &lt;- beta_hat + lr*grad
}

# And let&#39;s look at how well we did.
rbind(true_beta = true_beta,
      beta_hat = beta_hat,
      starting_beta_hat = starting_beta_hat)</code></pre>
<pre><code>##                         [,1]         [,2]      [,3]      [,4]
## true_beta           4.000000  0.000000000  3.000000  3.000000
## beta_hat            3.995367  0.007189571  3.025938  3.054183
## starting_beta_hat -10.803660 -4.528135234 -5.488053 -9.580921</code></pre>
<p>This is not too bad considering the coarse learning rate (which may jump over optimal solutions) and the relatively low number of iterations! Why don‚Äôt we try stochastic gradient descent as well? Here it is using the same data and initial values.</p>
<pre class="r"><code>sgd_beta_hat &lt;- starting_beta_hat
n_epochs &lt;- 100
lr &lt;- 1e-2

for (epoch in 1:n_epochs) {
  # Split thed data into batches
  batches &lt;- split(sample.int(n), seq(1, n, 4))
  
  # Update for each batch
  for (batch in batches) {
    # Get predictions using the current weights
    y_hat &lt;- sigm(x[batch, ], sgd_beta_hat)
    
    # Get the gradient
    grad &lt;- c(t(x[batch, ]) %*% (y[batch] - y_hat))
    
    # And update the parameters
    sgd_beta_hat &lt;- sgd_beta_hat + lr*grad
  }
}

# And let&#39;s look at how well we did.
rbind(true_beta = true_beta,
      beta_hat = beta_hat,
      sgd_beta_hat  = sgd_beta_hat,
      starting_beta_hat = starting_beta_hat)</code></pre>
<pre><code>##                         [,1]         [,2]      [,3]      [,4]
## true_beta           4.000000  0.000000000  3.000000  3.000000
## beta_hat            3.995367  0.007189571  3.025938  3.054183
## sgd_beta_hat        4.043590  0.122657665  3.020824  3.027758
## starting_beta_hat -10.803660 -4.528135234 -5.488053 -9.580921</code></pre>
<p>SGD should run faster because we are doing fewer operations per update (though the total number of iterations, number of batches * number of epochs, is larger).</p>
</div>
<div id="implement-a-perceptron" class="section level3">
<h3>Implement a Perceptron</h3>
<p>The Perceptron is a is a seminal classification method which attempts to learn <em>a</em> decision boundary for binary classification problems. For the perceptron class labels are either <span class="math inline">\(-1\)</span> or <span class="math inline">\(1\)</span>, and the goal is to minimize (using Hastie‚Äôs notation from Elements of Statistical Learning) the sum of the missclassified points <span class="math inline">\(-\sum_{i\in M} y_i (x_i^T\beta + \beta_0)\)</span>. The gradient (again from ESL) is just <span class="math inline">\(-\sum_{i\in M}y_ix_i\)</span>. We can optimize using stochastic gradient descent (i.e.¬†we can update <em>online</em>).</p>
<pre class="r"><code># Generate some data
n &lt;- 500
x &lt;- rbind(cbind(rnorm(n/2, mean = 2.25), rnorm(n/2, mean = 2.25)),
           cbind(rnorm(n/2, mean = -2.25), rnorm(n/2, mean = -2.25)))
y &lt;- ifelse(x[ , 1] + x[ , 2] &gt; 0, -1, 1)

plot(x, col = ifelse(y &gt; 0, &quot;blue&quot;, &quot;black&quot;))</code></pre>
<p><img src="/post/data-science-interview-questions_files/figure-html/unnamed-chunk-7-1.png" width="672" /></p>
<p>Great, now let‚Äôs implement the algorithm! Note that I won‚Äôt be including an intercept term here, which will force the decision boundary to pass through the origin. The data is generated about (0, 0) so this won‚Äôt really matter.</p>
<pre class="r"><code># Define a simple function that can update weights beta
# given a _vector_ x and a length 1 label y.
# So each call to perceptron will run one step of the algorithm by looking at
# one data point.
perceptron &lt;- function(x, y, beta, lr = 1) {
  # Get the prediction
  y_hat &lt;- sign(t(beta) %*% x)[1]
  # Update the betas if incorrect
  beta &lt;- beta + (y != y_hat)*lr*(y*x)
  # Return
  beta
}

accuracy &lt;- function(x, y, beta) {
  y_hat &lt;- apply(x, 1, function(.x) sign(t(beta) %*% .x))
  mean(y == y_hat)
}</code></pre>
<p>And now we can run the algorithm.</p>
<pre class="r"><code># Initialize some very wrong weights.
beta &lt;- c(1000, 1000)

# Should be 0.
print(paste0(&quot;Initial Accuracy: &quot;, accuracy(x, y, beta)))</code></pre>
<pre><code>## [1] &quot;Initial Accuracy: 0&quot;</code></pre>
<pre class="r"><code># Iterate over the data
n_epochs &lt;- 100
for (epoch in 1:n_epochs) {
  # Shuffle data
  shuffle &lt;- sample(1:nrow(x))
  x &lt;- x[shuffle, ]
  y &lt;- y[shuffle]
  
  # Update, one observation at a time
  # Note that I&#39;m letting the learning rate step down so that it can
  # initially take very large steps, then smaller and smaller steps.
  # I&#39;m not actually sure if this helps..anything.
  for (row in nrow(x)) {
    beta &lt;- perceptron(x[row, ], y[row], beta, lr = n_epochs/epoch)
  }
  
  # Progress report
  if (epoch %% 25 == 0)
    print(paste0(&quot;Epoch: &quot;, epoch, &quot;; Accuracy: &quot;, accuracy(x, y, beta)))
}</code></pre>
<pre><code>## [1] &quot;Epoch: 25; Accuracy: 0.002&quot;
## [1] &quot;Epoch: 50; Accuracy: 0.988&quot;
## [1] &quot;Epoch: 75; Accuracy: 0.988&quot;
## [1] &quot;Epoch: 100; Accuracy: 0.988&quot;</code></pre>
<p>Let‚Äôs take a look at how well the classification worked. Red points are classified incorrectly.</p>
<pre class="r"><code>y_hat &lt;- apply(x, 1, function(.x) sign(t(beta) %*% .x))
plot(x, col = ifelse(y != y_hat, &quot;red&quot;, ifelse(y &gt; 0, &quot;blue&quot;, &quot;black&quot;)))</code></pre>
<p><img src="/post/data-science-interview-questions_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<p>Note that for a perceptron to work the data must be <a href="https://en.wikipedia.org/wiki/Linear_separability">linearly separable</a>, but multilayer perceptrons can learn non-linear boundaries.</p>
</div>
<div id="implement-a-decision-tree" class="section level3">
<h3>Implement a Decision Tree</h3>
</div>
<div id="implement-adaboost" class="section level3">
<h3>Implement AdaBoost</h3>
</div>
<div id="implement-regularized-regression" class="section level3">
<h3>Implement Regularized Regression</h3>
</div>
<div id="implement-a-single-layer-perceptron" class="section level3">
<h3>Implement a Single Layer Perceptron</h3>
</div>
</div>

    <a href="/"> >> Home</a>
  </main>
</div>
    <footer>
      
<script>
(function() {
  function center_el(tagName) {
    var tags = document.getElementsByTagName(tagName), i, tag;
    for (i = 0; i < tags.length; i++) {
      tag = tags[i];
      var parent = tag.parentElement;
      
      if (parent.childNodes.length === 1) {
        
        if (parent.nodeName === 'A') {
          parent = parent.parentElement;
          if (parent.childNodes.length != 1) continue;
        }
        if (parent.nodeName === 'P') parent.style.textAlign = 'center';
      }
    }
  }
  var tagNames = ['img', 'embed', 'object'];
  for (var i = 0; i < tagNames.length; i++) {
    center_el(tagNames[i]);
  }
})();
</script>

<script src="//yihui.org/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

      
      <hr/>
      <a href="https://github.com/ben-e">Github</a> | <a href="https://www.linkedin.com/in/ben-e/">LinkedIn</a> | Built with Hugo üßô
      
    </footer>
  </body>
</html>

