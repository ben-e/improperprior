"0","# R"
"0",""
"0","# Sigmoid function is useful, this assumes x is a matrix"
"0","sigm <- function(x, beta) {"
"0","  1/(1 + exp(-apply(x, 1, function(.x) .x %*% beta)))"
"0","}"
"0",""
"0","# First, create some data!"
"0","n <- 10000"
"0","p <- 4"
"0","x <- cbind(rep(1, n), replicate(p - 1, rnorm(n)))"
"0","true_beta <- rpois(p, 2.5)"
"0","y <- rbinom(n, 1, sigm(x, true_beta))"
"0",""
"0","# Now we can begin our search, need to initalize beta, determine number of iterations of gradient"
"0","# descent, and choose a learning rate, these are by no means optimal choices."
"0","beta_hat <- rnorm(p, 0, 10)"
"0","starting_beta_hat <- beta_hat"
"0","n_iters <- 1000"
"0","lr <- 1e-3"
"0",""
"0","for (iter in 1:n_iters) {"
"0","  # Get predictions using the current weights"
"0","  y_hat <- sigm(x, beta_hat)"
"0","  "
"0","  # Get the gradient"
"0","  grad <- c(t(x) %*% (y - y_hat))"
"0","  "
"0","  # And update the parameters"
"0","  beta_hat <- beta_hat + lr*grad"
"0","}"
"0",""
"0","# And let's look at how well we did."
"0","rbind(true_beta = true_beta,"
"0","      beta_hat = beta_hat,"
"0","      starting_beta_hat = starting_beta_hat)"
"1","                 "
"1","       [,1]"
"1","       [,2]"
"1","     [,3]"
"1","    [,4]"
"1","
true_beta        "
"1","   4.000000"
"1","   5.000000"
"1"," 2.000000"
"1"," 3.00000"
"1","
beta_hat         "
"1","   4.104327"
"1","   5.133133"
"1"," 2.040719"
"1"," 3.10322"
"1","
starting_beta_hat"
"1"," -11.771508"
"1"," -10.567567"
"1"," 5.889800"
"1"," 2.69979"
"1","
"
