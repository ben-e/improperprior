---
title: "What is an improper prior?"
description: |
  A short introduction to the use of improper priors in Bayesian statistics.
author:
  - name: Ben Ewing
    url: https://improperprior.com/
date: 2020-03-28
output:
  distill::distill_article:
    self_contained: false
---

```{r r-setup-packages, echo=F}
# Packages
# Data manipulation
library(dplyr)
library(purrr)
library(tidyr)

# Data viz
library(knitr)
library(ggplot2)
library(gganimate)
library(ggcute)
```

```{r r-setup-settings, echo=F}
# Settings
opts_chunk$set(echo = F,
               dev.args = list(bg = "transparent"))

theme_set(theme_void(base_family = "monospace"))
theme_update(panel.background = element_rect(fill = "transparent", 
                                             colour = NA),
             plot.background = element_rect(fill = "transparent", 
                                            colour = NA))
```

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "AMS"
      } 
  }
});
</script>

# Introduction

While I've purchased [improperprior.com](https://improperprior.com/) as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.

This post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.

# Priors As Usual

In the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you're into [Empirical Bayes](https://en.wikipedia.org/wiki/Empirical_Bayes_method)), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.

This process is powered by [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). For example, suppose we have data $Y$ that follow a [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution), and we would like to estimate the $\theta$ parameter. Using Bayes' theorem we would compute:

\begin{equation}
(\#eq:bayes)
P(\theta|Y) = \frac{L(Y|\theta)P(\theta)}{P(Y)}.
\end{equation}

Where:

* $P(\theta|Y)$ is the posterior probability, this is what we'd like to estimate.
* $L(Y|\theta)$ is the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) of observing the data, given $\theta$.
* $P(\theta)$ is a distribution representing a prior guess for $\theta$ before observing data.
* $P(Y)$ is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a _proper_ distribution), which is surprisingly unnecessary for posterior estimation.

The canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it's potential shapes. 

```{r beta-distribution-demo}
# I basically stole this from wikipedia.

# Generate data
step <- 0.05
x <- seq(0, 1, 0.01)
df <- bind_rows(
  tibble(alpha = seq(step, 5, step), beta = 5),
  tibble(alpha = 5, beta = seq(5, step, -step)),
  tibble(alpha = 5, beta = seq(step, 5, step)),
  tibble(alpha = seq(5, step, -step), beta = alpha),
  tibble(alpha = seq(step, 5, step), beta = alpha),
  tibble(alpha = seq(5, step, -step), beta = 5),
) %>% 
  mutate(id = 1:n()) %>% 
  pmap_dfr(
    function(alpha, beta, id) 
      tibble(alpha = alpha, beta = beta, id = id, x = x,
             density = dbeta(x, alpha, beta))
  )

# Build the plot(s)
beta_example <- ggplot(df, aes(x, density)) +
  geom_line(size = 1.25, colour = "#E3837D") +
  geom_text(x = 0.5, y = 4, 
            aes(label = paste0("alpha: ", round(alpha, 2), 
                               "\n beta: ", round(beta, 2)))) +
  xlim(c(0, 1)) +
  ylim(c(0, 5)) +
  transition_manual(id)

# Animation settings
animate(
  beta_example,
  bg = 'transparent',
  width = 6,
  height = 2,
  units = "in",
  res = 300,
  nframes = 150
)
```

For this example we'll use a flat prior, which gives equal weight to all possible values of $\theta$. The $\text{Beta}(1, 1)$ distribution does this by just giving a Uniform distribution over $[0, 1]$.

The likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just $\theta^{\sum_i y_i}(1-\theta)^{n-\sum_i y_i}$. We can complete the numerator of \@ref(eq:bayes) by combining this likelihood with the $\text{Beta}(1, 1)$ prior, which gives: 

\begin{equation}
(\#eq:posterior)
P(\theta|Y) \propto \theta^{\sum_i y_i}(1-\theta)^{n-\sum_i y_i} \times \theta^{1-1}(1-\theta)^{1-1} \\
            \propto \theta^{\sum_i y_i + 1 - 1}(1-\theta)^{n-\sum_i y_i  + 1 - 1}.
\end{equation}

From this we can see that the posterior distribution follows a $\text{Beta}(\sum_i y_i + 1, n - \sum_i y_i + 1)$ distribution. Intuitively  because we used a prior that carried little information with it (effectively saying that $\theta$ could be any value in (0, 1)) our posterior estimate for $\theta$ is entirely determined by the data.

Here's a simple Shiny app to let you play with this model and build some intuition as to how the prior parameters and data interact.

```{r, fig.width = 15, echo = F}
knitr::include_app("https://ben-ewing.shinyapps.io/Beta-Binomial/", height = 465)
```

# Limitations

So far this seems like a great framework, but the requirement that the prior be a proper distribution can be quite restrictive. Consider the normal distribution (for simplicity, assume known $\sigma^2$): $\mu$ can take _any_ real value, so how can we use a flat prior with equal probability for each possible value of $\mu$?

<aside>
Note that a proper distribution is one with a density function that integrates to 1.
</aside>

While powerful in specific cases, Bayesian modeling is rather limited if we can only use proper distributions as priors.

# Improper Priors

Naively, what would happen if we just set the probability of each $\mu$ to 1? Well it turns out that we can do exactly this - we can use any prior, even an _improper prior_ as long as the posterior comes out to be a proper distribution.

Choosing an improper prior that generates a valid posterior can be a tricky affair, but using [Jeffreys' prior](https://en.wikipedia.org/wiki/Jeffreys_prior) is a good place to start. Continuing the normal example, we will just use a prior probability of 1 for every value of $\mu$. This is actially proportional to the [Jeffreys' prior for this setup](https://en.wikipedia.org/wiki/Jeffreys_prior#Gaussian_distribution_with_mean_parameter). As with the previous example, we will set aside all constant terms:

$$
P(\theta|Y) \propto \exp{\left[-\frac{1}{2} \left(\frac{y-\mu}{\sigma^2}\right)^2\right]} \times 1.
$$

The posterior is just a proper normal distribution!

It is of course possible to go much deeper into improper priors, particularly choosing a good prior, but as far as the concept goes, this is mostly all there is to it!

## Further Resources

[Craig Gidney](https://algassert.com/post/1630) has a nice blog post walking through a slightly more technical example of improper priors. Likewise, [Andy Jones](https://andrewcharlesjones.github.io/journal/improper-priors.html) has a great podcast with a few additional examples. For a more general treatment [A First Course in Bayesian Statistical Methods](https://duckduckgo.com/?t=ffab&q=A+First+Course+in+Bayesian+Statistical+Methods&atb=v198-1&ia=shopping) by Hoff and [Bayesian Data Analysis](https://duckduckgo.com/?q=bayesian+data+analysis&t=ffab&atb=v198-1&ia=shopping) by Gelman et al are the standard introductory Bayesian statistics textbooks.

As ever, Wikipedia has very detailed articles on priors, more suitable for reference than learning:

* [Improper Priors](https://en.wikipedia.org/wiki/Prior_probability#Improper_priors)
* [Jeffreys' prior](https://en.wikipedia.org/wiki/Jeffreys_prior)
* [Conjugate Priors](https://en.wikipedia.org/wiki/Conjugate_prior)


# Further Resources
