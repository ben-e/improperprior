---
title: "What is an improper prior?"
description: |
  A short description of the post.
author:
  - name: Ben Ewing
    url: https://improperprior.com/
date: 2020-03-28
output:
  distill::distill_article:
    self_contained: false
---

```{r r-setup-packages, echo=F}
# Packages
# Data manipulation
library(dplyr)
library(purrr)
library(tidyr)

# Data viz
library(knitr)
library(ggplot2)
library(gganimate)
library(ggcute)
```

```{r r-setup-settings, echo=F}
# Settings
opts_chunk$set(echo = F,
               dev.args = list(bg = "transparent"))

theme_set(theme_void(base_family = "monospace"))
theme_update(panel.background = element_rect(fill = "transparent", 
                                             colour = NA),
             plot.background = element_rect(fill = "transparent", 
                                            colour = NA))
```

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { 
      equationNumbers: { 
            autoNumber: "AMS"
      } 
  }
});
</script>

# Introduction

While I've purchased [improperprior.com](https://improperprior.com/) as my personal website, it seems irresponsible not to write a post giving a short explaining improper priors.

This post was originally published on 2020-03-28, it was updated on 2021-07-24 to improve formatting and clarity.

# Priors As Usual

In the Bayesian framework we presume our data is generated by some distribution with a given set of parameters. Further, we assume that the parameters themselves are drawn from a distribution called a prior. Typically the prior is set before observing any data (unless you're into [Empirical Bayes](https://en.wikipedia.org/wiki/Empirical_Bayes_method)), and then observed data is used to generate a new distribution, the posterior, which is said to be a compromise between the prior and the observed data.

This process is powered by [Bayes' theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem). For example, suppose we have data $Y$ that follow a [Binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution), and we would like to estimate the $\theta$ parameter. Using Bayes' theorem we would compute:

\begin{equation}
(\#eq:bayes)
P(\theta|Y) = \frac{L(Y|\theta)P(\theta)}{P(Y)}.
\end{equation}

Where:

* $P(\theta|Y)$ is the posterior probability, this is what we'd like to estimate.
* $L(Y|\theta)$ is the [likelihood](https://en.wikipedia.org/wiki/Likelihood_function) of observing the data, given $\theta$.
* $P(\theta)$ is a distribution representing a prior guess for $\theta$ before observing data.
* $P(Y)$ is the unconditional likelihood of observing the data, also commonly called a normalizing constant. We will ignore this term as it is constant once the data has been observed, and only acts to make sure that the numerator integrates to 1 (i.e. it makes sure the posterior is a _proper_ distribution), which is surprisingly unnecessary for posterior estimation.

The canonical prior for data from a binomial distribution is the beta distribution. This is for good reason, the beta distribution has support between 0 and 1 (bounded or not!) and is very versatile with respect to it's potential shapes. 

```{r beta-distribution-demo}
# I basically stole this from wikipedia.

# Generate data
step <- 0.05
x <- seq(0, 1, 0.01)
df <- bind_rows(
  tibble(alpha = seq(step, 5, step), beta = 5),
  tibble(alpha = 5, beta = seq(5, step, -step)),
  tibble(alpha = 5, beta = seq(step, 5, step)),
  tibble(alpha = seq(5, step, -step), beta = alpha),
  tibble(alpha = seq(step, 5, step), beta = alpha),
  tibble(alpha = seq(5, step, -step), beta = 5),
) %>% 
  mutate(id = 1:n()) %>% 
  pmap_dfr(
    function(alpha, beta, id) 
      tibble(alpha = alpha, beta = beta, id = id, x = x,
             density = dbeta(x, alpha, beta))
  )

# Build the plot(s)
beta_example <- ggplot(df, aes(x, density)) +
  geom_line(size = 1.25, colour = "#E3837D") +
  geom_text(x = 0.5, y = 4, 
            aes(label = paste0("alpha: ", round(alpha, 2), 
                               "\n beta: ", round(beta, 2)))) +
  xlim(c(0, 1)) +
  ylim(c(0, 5)) +
  transition_manual(id)

# Animation settings
animate(
  beta_example,
  bg = 'transparent',
  width = 6,
  height = 2,
  units = "in",
  res = 300,
  nframes = 150
)
```

For this example we'll use a flat prior, which gives equal weight to all possible values of $\theta$. The $\text{Beta}(1, 1)$ distribution does this by just giving a Uniform distribution over $[0, 1]$.

The likelihood is just the probability that we observe the data sampled data. The likelihood for binomial data is just a binomial distribution itself. Setting aside constant terms the likelihood is just $\theta^{\sum_i y_i}(1-\theta)^{n-\sum_i y_i}$. We can complete the numerator of \@ref(eq:bayes) by combining this likelihood with the $\text{Beta}(1, 1)$ prior, which gives: 

\begin{equation}
(\#eq:posterior)
P(\theta|Y) \propto \theta^{\sum_i y_i}(1-\theta)^{n-\sum_i y_i} \times \theta^{1-1}(1-\theta)^{1-1} \\
            \propto \theta^{\sum_i y_i + 1 - 1}(1-\theta)^{n-\sum_i y_i  + 1 - 1}.
\end{equation}

# Limitations

# Improper Priors

# Further Resources
